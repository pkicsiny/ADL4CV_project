{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get rain data and visualise for overview <font color='orange'>&#9728;</font>\n",
    "- cut and split data to reasonable train, val, test sets <font color='orange'>&#9728;</font>\n",
    "- set up first generator and discriminator and overfit, play around with it, get first results (_November/December_)\n",
    "- train on small training set and improve losses, accuracy, results (_December_)\n",
    "- experiment with temporal discriminator (_December/January_)\n",
    "- experiment with wind data (_January_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forces CPU usage\n",
    "environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________-\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain measurements\n",
    "Measurements are downloaded from the DWD (German weather service) open data server: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/__<br>\n",
    "I'm working with the data of August 2010 (based on [this](https://tradingeconomics.com/germany/precipitation)), so I have downloaded this: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/2010/RW-201008.tar__<br>\n",
    "_DWD manual:_<br>\n",
    "__ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/BESCHREIBUNG_gridsgermany_hourly_radolan_historical_asc_de.pdf__<br><br>\n",
    "This contains radar maps recorded in every hour. Each map has a resolution of $900\\times900$ pixels and each pixel corresponds to an $1\\,km\\times1\\,km$ area in reality. Pixel values are the precipitation height in $0.1\\,mm$.\n",
    "Below I'm importing the data of this as a series of numpy arrays and plot them to see the acual radar map. The _sys.path[0]_ is the path on my computer and it can be different for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = src.get_data(sys.path[0]+\"/rain\", total_length=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = src.generate_datasets(inputs, n=10, size=64, length=2, normalize=True, split=(6,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_res_train = images[\"low_res_train\"]\n",
    "#low_res_xval = images[\"low_res_xval\"]\n",
    "#low_res_test = images[\"low_res_test\"]\n",
    "#overfit = np.reshape(images['images'],np.shape(images['images'])+(1,))\n",
    "train = np.reshape(images[\"train\"],np.shape(images[\"train\"])+(1,))\n",
    "xval = np.reshape(images[\"xval\"],np.shape(images[\"xval\"])+(1,))\n",
    "test = np.reshape(images[\"test\"],np.shape(images[\"test\"])+(1,))\n",
    "print(f\"Training data: {train.shape}\\nValidation data: {xval.shape}\\nTest data: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "src.visualise_data(xval[:,:,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential()\n",
    "\n",
    "init = keras.layers.Input(shape=(1,32,32))\n",
    "#32\n",
    "UpSamp1 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_first\")(init)\n",
    "Conv1   = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(1,1),padding=\"same\", data_format=\"channels_first\")(UpSamp1)\n",
    "Lr1     = keras.layers.LeakyReLU(alpha=0.0)(Conv1)\n",
    "#64\n",
    "UpSamp2 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_first\")(Lr1)\n",
    "Conv2   = keras.layers.Conv2D(filters=32,kernel_size=4,strides=1,padding=\"same\", data_format=\"channels_first\")(UpSamp2)\n",
    "Lr2     = keras.layers.LeakyReLU(alpha=0.0)(Conv2)\n",
    "\n",
    "Conv3   = keras.layers.Conv2D(filters=16,kernel_size=4,strides=1,padding=\"same\", data_format=\"channels_first\")(Lr2)\n",
    "Lr3     = keras.layers.LeakyReLU(alpha=0.0)(Conv3)\n",
    "\n",
    "Conv4   = keras.layers.Conv2D(filters=8,kernel_size=(4,4),strides=(1,1),padding=\"same\",\n",
    "                              activation = 'relu', data_format=\"channels_first\")(Lr3)\n",
    "Conv5   = keras.layers.Conv2D(filters=1,kernel_size=(4,4),strides=(1,1),padding=\"same\",\n",
    "                              activation = 'elu', data_format=\"channels_first\")(Conv4)\n",
    "\n",
    "model = keras.models.Model(inputs=init, outputs=Conv5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfit\n",
    "overfit = np.expand_dims(low_res_train[0],axis=0)\n",
    "overfit_label = np.expand_dims(train[0],axis=0)\n",
    "np.shape(overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss='mean_absolute_error', metrics=['accuracy']) \n",
    "model.fit(overfit, overfit_label,\n",
    "          validation_data=(overfit,overfit_label),\n",
    "          batch_size = 1,\n",
    "          epochs=350,\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.history\n",
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = model.predict(low_res_test, batch_size=10)\n",
    "truth       = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images, error_vals = src.error_distribution(truth,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src.upsample_plotter(np.linspace(1,10,10),(truth, low_res_test, predictions, error_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batch_size=10\n",
    "lstm_model = keras.Sequential()\n",
    "# define CNN model\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=8, kernel_size=5, strides=2, padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=16, kernel_size=5, strides=2, padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=5, strides=2, padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=5, strides=2, padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=128, kernel_size=5, strides=4, padding='same', activation='tanh')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "\n",
    "#LSTM\n",
    "lstm_model.add(keras.layers.LSTM(units=128, return_sequences=True))\n",
    "\n",
    "#upconv\n",
    "lstm_model.add(keras.layers.Reshape((1,1,1,128)))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=64,kernel_size=5,strides=4,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=32,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=16,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=8,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=1,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "\n",
    "lstm_model.build((None,1,)+train.shape[2:])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train     = np.reshape(train[:,0,:,:,:],((train.shape[0],1,)+train.shape[2:]))\n",
    "lstm_truth     = np.reshape(train[:,1,:,:,:],((train.shape[0],1,)+train.shape[2:]))\n",
    "lstm_val       = np.reshape(xval[:,0,:,:,:],((xval.shape[0],1,)+xval.shape[2:]))\n",
    "lstm_val_truth = np.reshape(xval[:,1,:,:,:],((xval.shape[0],1,)+xval.shape[2:]))\n",
    "print(lstm_train.shape,\"\\n\",lstm_truth.shape,\"\\n\",lstm_val.shape,\"\\n\",lstm_val_truth.shape)\n",
    "lstm_test      = np.reshape(test[:,0,:,:,:],((test.shape[0],1,)+test.shape[2:]))\n",
    "lstm_test_truth = np.reshape(test[:,1,:,:,:],((test.shape[0],1,)+test.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_train = np.reshape(train[3,0,:,:,:],((1,1,)+train.shape[2:]))\n",
    "overfit_truth = np.reshape(train[3,1,:,:,:],((1,1,)+train.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss=\"mean_absolute_error\", metrics=[src.relative_error_tensor]) \n",
    "#train the model\n",
    "lstm_model.fit(overfit_train, overfit_truth,\n",
    "               validation_data=(lstm_val, lstm_val_truth),\n",
    "               batch_size = 1,\n",
    "               epochs=500,\n",
    "               shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = lstm_model.history\n",
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig('training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = lstm_model.predict(overfit_train, batch_size=1)\n",
    "truth       = overfit_truth\n",
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = src.arg_getter(truth, predictions)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_images, error_vals, error_means = src.error_distribution(truth,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src.result_plotter([0], (overfit_train[:,:,:,:,0], truth[:,:,:,:,0], predictions[:,:,:,:,0], error_images[:,:,:,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = keras.Sequential()\n",
    "depth = 2\n",
    "dropout = 0.4\n",
    "\n",
    "conv1 = keras.layers.Conv2D(filters=depth*1 ,kernel_size=10, strides=5, input_shape=(900,900,1), padding='same')\n",
    "relu1 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout1 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv2 = keras.layers.Conv2D(filters=depth*2, kernel_size=10, strides=5, padding='same')\n",
    "relu2 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout2 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv3 = keras.layers.Conv2D(filters=depth*4, kernel_size=6, strides=2, padding='same')\n",
    "relu3 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout3 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv4 = keras.layers.Conv2D(filters=depth*8, kernel_size=2, strides=2, padding='same')\n",
    "relu4 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout4 = keras.layers.Dropout(dropout)\n",
    "\n",
    "# Out: 1-dim probability\n",
    "flatten = keras.layers.Flatten()\n",
    "fcl1 = keras.layers.Dense(1)\n",
    "sig1 = keras.layers.Activation('sigmoid')\n",
    "#summary()\n",
    "D.add(conv1)\n",
    "D.add(relu1)\n",
    "D.add(dropout1)\n",
    "D.add(conv2)\n",
    "D.add(relu2)\n",
    "D.add(dropout2)\n",
    "D.add(conv3)\n",
    "D.add(relu3)\n",
    "D.add(dropout3)\n",
    "D.add(conv4)\n",
    "D.add(relu4)\n",
    "D.add(dropout4)\n",
    "D.add(flatten)\n",
    "D.add(fcl1)\n",
    "D.add(sig1)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=3e-8)\n",
    "AM = keras.Sequential()\n",
    "AM.add(G)\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM.fit(train_val_inputs,\n",
    "          train_val_targets,\n",
    "          batch_size = 50,\n",
    "          epochs=50,\n",
    "          validation_split = 0.1,\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
