{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the datasets from here: https://drive.google.com/open?id=1mNEblJS0622w5-2mB6yCyMbu7RbcfENL<br>\n",
    "This should contain the following:\n",
    "- __5_min_train__: training data as np array. Shape: (7500, 64, 64, 8)\n",
    "- __5_min_xval__: validation data (currently unused) as np array. Shape: (1500, 64, 64, 8)\n",
    "- __5_min_test__: test data (used as visual validation during training) as np array. Shape: (1000, 64, 64, 8)\n",
    "- __5_min_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence. Shape: (10000,)\n",
    "- __5_min_long_pred__: test data for sequence prediction as np array. We used it for testing after training. Shape: (1000, 64, 64, 20)\n",
    "- __5_min_long_pred_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence for the 5_min_long_pred dataset. Shape: (1000,)\n",
    "- __tgan_1/2/4-1_vx/vy_2000__: optical flow images between the last and second last frames of the input for the first 2000 sequences of the training dataset (__5_min_train__) as np array. The 1/2/4 means the length of the input sequence. We mostly used 2. Shape: (2000, 64, 64, 1)\n",
    "- __germany__: Not needed. (GPS coordinates of Germany. Used for experimenting before.)\n",
    "\n",
    "In the datasets the first axis is stands for the sample, the next two for the frame height and width and the last for the channels which is the time axis here.<br>\n",
    "If data is missing or you cannot acces the drive, please write me an E-Mail: pkicsiny@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from functools import partial\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to src.py\n",
    "spec = importlib.util.spec_from_file_location(\"src.py\", \"C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/src.py\")\n",
    "src = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(src)\n",
    "\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/models/')\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "PAST is about the number of before frame. As we presented in 2nd presentation, our reference paper used 2 frames for input data. And we followed them. Also we augmented the training data by rotating the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAST = 2\n",
    "name = \"sgan_2-1_iw\"\n",
    "#set this to true if you want to use a pretrained model and load its weights from file\n",
    "use_loaded=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = src.load_datasets(past_frames=PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, T_train, X_val, T_val, X_test, T_test = src.split_datasets(\n",
    "            train[:2000], val, test, past_frames=PAST, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the our data\n",
    "plt.imshow(X_train[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to save images, and plot the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is to save the predicted image for every nth iteration.\n",
    "def sample_images(epoch, gan_test, gan_test_truth, generator, past_input, save_path):\n",
    "    # epoch: the iteration number of training\n",
    "    # gan_test: input data frames for testing\n",
    "    # gan_test_truth: output data frame for testing\n",
    "    # generator: trained generator model\n",
    "    # past_input: the number of frames for input data\n",
    "    # save_path: directory path to save \n",
    "    n = 5\n",
    "    test_batch = gan_test[:n]\n",
    "    test_truth = gan_test_truth[:n]\n",
    "    gen_imgs = generator.predict(test_batch)\n",
    "    plot_range = past_input \n",
    "    fig, axs = plt.subplots(n, plot_range+2, figsize=(16, 16))\n",
    "    for i in range(n):\n",
    "        vmax = np.max([np.max(test_batch[i]), np.max(test_truth[i])])\n",
    "        vmin = 0\n",
    "        for j in range(plot_range):\n",
    "            im = axs[i,j].imshow(test_batch[i, :,:,j], vmax=vmax,vmin=vmin)\n",
    "            axs[i,j].axis('off')\n",
    "            src.colorbar(im)\n",
    "            axs[i,j].set_title(\"Frame t\"+str([-past_input+1+j if j < past_input-1 else \"\"][0]))\n",
    "        im2 = axs[i,-2].imshow(test_truth[i, :,:,0], vmax=vmax, vmin=vmin)\n",
    "        axs[i,-2].axis('off')\n",
    "        src.colorbar(im2)                \n",
    "        axs[i,-2].set_title(\"Frame t+1\")\n",
    "        im3 = axs[i,-1].imshow(gen_imgs[i, :,:,0], vmax=vmax, vmin=vmin)\n",
    "        axs[i,-1].axis('off')\n",
    "        src.colorbar(im3)\n",
    "        axs[i,-1].set_title(\"Prediction t+1\")\n",
    "    fig.savefig(save_path+'epoch'+str(epoch)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is to save the training curve for every nth iteration.\n",
    "def plot_training_curves(log, epoch, name, wgan=False, gp=False):\n",
    "    # log: a dictionary variable for losses\n",
    "    # epoch: the iteration number of training\n",
    "    # name: directory path to save \n",
    "    # wgan: is used for wgan(-gp) model or not\n",
    "    # gp: is used for wgan-gp model or not\n",
    "    total_g_loss = np.array(log[\"g_loss\"])[:, 0]\n",
    "    if gp == True and wgan == True : \n",
    "        total_d_loss = np.array(log[\"d_loss\"])[:, 0]\n",
    "    elif gp == False and wgan == True:\n",
    "        total_d_loss = np.array(log[\"d_loss\"])\n",
    "    elif wgan == False :\n",
    "        total_d_loss = np.array(log[\"d_loss\"])[:, 0]\n",
    "    smoothed_tgl = src.smooth(np.array(log[\"g_loss\"])[:, 0])\n",
    "    smoothed_tdl = src.smooth(total_d_loss)\n",
    "    objective_loss = np.array(log[\"g_loss\"])[:, 1]\n",
    "\n",
    "    f, (a0, a1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [5, 2]})\n",
    "    a0.plot(total_g_loss, alpha=0.3, c=\"b\")\n",
    "    a0.plot(smoothed_tgl, c=\"b\", label=\"generator\")\n",
    "    a0.grid()\n",
    "    if wgan:\n",
    "        a0.plot(np.array(log[\"d_loss_real\"]), c=\"g\", label=\"real\")\n",
    "        a0.plot(np.array(log[\"d_loss_fake\"]), c=\"r\", label=\"fake\")\n",
    "    else:\n",
    "        a0.plot(total_d_loss, alpha=0.3, c=\"orange\")\n",
    "        a0.plot(smoothed_tdl, c=\"orange\", label=\"discriminator\")\n",
    "    a0.legend()\n",
    "    a1.plot(objective_loss, alpha=0.9, c=\"green\", label=\"L1 objective\")\n",
    "    a1.grid()\n",
    "    a1.legend()\n",
    "    f.text(0.5, 0, 'Iterations', ha='center', va='center')\n",
    "    f.text(0, 0.5, 'Loss', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(name+'epoch_'+str(epoch)+'_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is to save the image with real test dataset (only once after whole training)\n",
    "def save_examples(path, test, predictions_dict, past, future, samples=0):\n",
    "    # path: directory path to save \n",
    "    # test: test dataset\n",
    "    # prediction_dict: predicted images by the generator\n",
    "    # past: the number of frames for input data\n",
    "    # future: the number of framse to predict\n",
    "    # samples: indexes of test dataset to save\n",
    "    fig, axs = plt.subplots(len(samples)*2,past+future, figsize=(32, 32))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.0)\n",
    "    for n in range(len(samples)):\n",
    "        vmax = np.max(test[n,:,:,:past])\n",
    "        vmin = 0\n",
    "        print(test.shape)\n",
    "        for i in range(past):\n",
    "            im = axs[2*n,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "        for i in range(past,past+future):\n",
    "            im = axs[2*n,i].imshow(predictions_dict[f\"{i-past}\"][samples[n], :,:,0], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Predicted frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Reference frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "    fig.savefig(path+\"_sequence_prediction.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN    \n",
    "    reference sources:\n",
    "* https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "* https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html\n",
    "* https://github.com/bobchennan/Wasserstein-GAN-Keras/blob/master/mnist_wacgan.py\n",
    "* https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special functions for WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from 3rd reference\n",
    "# Loss function for Wasserstein GAN (WGAN)\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from 4nd reference\n",
    "# Additional loss function for improved Wasserstein GAN (WGAN-GP)\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "    \"\"\"Calculates the gradient penalty loss for a batch of \"averaged\" samples.\n",
    "    In Improved WGANs, the 1-Lipschitz constraint is enforced by adding a term to the loss function\n",
    "    that penalizes the network if the gradient norm moves away from 1. However, it is impossible to evaluate\n",
    "    this function at all points in the input space. The compromise used in the paper is to choose random points\n",
    "    on the lines between real and generated samples, and check the gradients at these points. Note that it is the\n",
    "    gradient w.r.t. the input averaged samples, not the weights of the discriminator, that we're penalizing!\n",
    "    In order to evaluate the gradients, we must first run samples through the generator and evaluate the loss.\n",
    "    Then we get the gradients of the discriminator w.r.t. the input averaged samples.\n",
    "    The l2 norm and penalty can then be calculated for this gradient.\n",
    "    Note that this loss function requires the original averaged samples as input, but Keras only supports passing\n",
    "    y_true and y_pred to loss functions. To get around this, we make a partial() of the function with the\n",
    "    averaged_samples argument, and use that for model training.\"\"\"\n",
    "    # first get the gradients:\n",
    "    #   assuming: - that y_pred has dimensions (batch_size, 1)\n",
    "    #             - averaged_samples has dimensions (batch_size, nbr_features)\n",
    "    # gradients afterwards has dimension (batch_size, nbr_features), basically\n",
    "    # a list of nbr_features-dimensional gradient vectors\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from 4nd reference\n",
    "# Funtion is to merge the ground truth and generated image with random weights\n",
    "class RandomWeightedAverage(keras.layers.Concatenate):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms, this outputs a random point on the line\n",
    "    between each pair of input points.\n",
    "    Inheriting from _Merge is a little messy but it was the quickest solution I could think of.\n",
    "    Improvements appreciated.\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet is used for Generator\n",
    "G = src.unet(X_train.shape[1:], dropout=0, batchnorm=True, kernel_size=4, feature_mult=1)\n",
    "if use_loaded:\n",
    "    G.load_weights(sys.path[1]+name+\"/G_model.h5\")\n",
    "# G.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using loaded weights for prediction only then please skip to the __prediction__ section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convoluntional classifier is used for Discriminator\n",
    "SD = src.spatial_discriminator(condition_shape=X_train.shape[1:], dropout = 0.25, batchnorm=True, wgan=True)\n",
    "# SD.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters for training\n",
    "# random seed\n",
    "RND = 777\n",
    "np.random.seed(RND)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "ITERATIONS = 50\n",
    "\n",
    "# we train the discriminator 100 times for first 25 iterations (every start of 500 iterations)\n",
    "# and normally train the discriminator 10 times for each iteration\n",
    "INIT_D_ITERS = 100\n",
    "D_ITERS = 10\n",
    "\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "#path to plot validation images\n",
    "PATH = '/Users/jlee/Desktop/JONG/TUM/18W/Advanced_Deep_Learning_for_Computer_Vision/project/results/wgan_results/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the discriminator's weights\n",
    "for l in SD.layers:\n",
    "    l.trainable = False\n",
    "SD.trainable = False\n",
    "\n",
    "# make a whole network to train the generator with freezed discriminator\n",
    "condition = keras.layers.Input(shape=X_train.shape[1:], name='input_condition_')\n",
    "generated = G(condition)\n",
    "output_is_fake = SD(inputs = [condition, generated])\n",
    "DG = keras.models.Model(inputs=[condition], outputs=[generated, output_is_fake])\n",
    "\n",
    "# keras.optimizers.Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "# in our reference code, they used Adam. But we found that RMSprop provides better results for our case.\n",
    "DG.compile(optimizer=keras.optimizers.RMSprop(lr=0.00005), \n",
    "           loss = [src.custom_loss(loss='l1'), wasserstein_loss],\n",
    "           loss_weights = [0,1]\n",
    ")\n",
    "DG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in SD.layers:\n",
    "    l.trainable = True\n",
    "for l in G.layers:\n",
    "    l.trainable = False\n",
    "SD.trainable = True\n",
    "G.trainable = False\n",
    "\n",
    "# make a network to train the discriminator with gradient penalty loss.\n",
    "real_samples = keras.layers.Input(shape=T_train.shape[1:])\n",
    "condition = keras.layers.Input(shape=X_train.shape[1:])\n",
    "generated = G(condition)\n",
    "d_output_generated = SD([condition, generated])\n",
    "d_output_real = SD([condition, real_samples])\n",
    "\n",
    "averaged_samples = RandomWeightedAverage()([real_samples, generated])\n",
    "averaged_samples_out = SD([condition, averaged_samples])\n",
    "\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                          averaged_samples=averaged_samples,\n",
    "                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' \n",
    "\n",
    "D = keras.models.Model(inputs=[condition, real_samples],\n",
    "          outputs=[d_output_real,\n",
    "                   d_output_generated,\n",
    "                   averaged_samples_out])\n",
    "\n",
    "# keras.optimizers.Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "# in our reference code, they used Adam. But we found that RMSprop provides more good results for our case.\n",
    "D.compile(optimizer=keras.optimizers.RMSprop(lr=0.00005),\n",
    "          loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss])\n",
    "\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warning message (because there are huge messages when you freeze weights)\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "progress_bar = Progbar(target=ITERATIONS)\n",
    "\n",
    "log = {\"g_loss\":[],\n",
    "       \"d_loss\":[],\n",
    "       \"d_loss_gp\":[],\n",
    "       \"d_loss_real\":[],\n",
    "       \"d_loss_fake\":[],\n",
    "       \"d_loss_wgan\":[]}\n",
    "\n",
    "positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for it in range(ITERATIONS+1):\n",
    "\n",
    "    if len(log['d_loss_real']) > 5:\n",
    "        progress_bar.update(\n",
    "            it,\n",
    "            values=[ # avg of 5 most recent\n",
    "                    ('d_loss_r', np.mean(log['d_loss_real'][-5:], axis=0)),\n",
    "                    ('d_loss_f', np.mean(log['d_loss_fake'][-5:], axis=0)),\n",
    "                    ('g_loss', np.mean(log['g_loss'][-5:],axis=0))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        progress_bar.update(it)\n",
    "        \n",
    "    # 1: train D on real+generated images\n",
    "\n",
    "    if (it % 500) < 25 or it % 250 == 0: # 25 times in 1000, every 500th\n",
    "        d_iters = INIT_D_ITERS\n",
    "    else:\n",
    "        d_iters = D_ITERS\n",
    "     \n",
    "    # freeze G\n",
    "    for l in SD.layers:\n",
    "        l.trainable = True\n",
    "    for l in G.layers:\n",
    "        l.trainable = False\n",
    "    SD.trainable = True\n",
    "    G.trainable = False\n",
    "\n",
    "    for d_it in range(d_iters):\n",
    "\n",
    "        # random samples from training dataset\n",
    "        index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "        base_images = X_train[index]\n",
    "        real_images = T_train[index]\n",
    "\n",
    "        # maximize D output on reals === minimize -1*(D(real))\n",
    "        # minimize D output on fakes === minimize 1*(D(fake))\n",
    "        D_loss = D.train_on_batch([base_images, real_images], [negative_y, positive_y, dummy_y])\n",
    "\n",
    "    # 2: train D(G) \n",
    "    \n",
    "    # freeze D\n",
    "    for l in SD.layers:\n",
    "        l.trainable = False\n",
    "    for l in G.layers:\n",
    "        l.trainable = True\n",
    "    SD.trainable = False\n",
    "    G.trainable = True\n",
    "        \n",
    "    # random samples from training dataset\n",
    "    index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "    base_images = X_train[index]\n",
    "    real_images = T_train[index]\n",
    "\n",
    "    # maximize D output on fakes === minimize -1*(D(fake))\n",
    "    DG_loss = DG.train_on_batch(base_images, [real_images, negative_y])\n",
    "    \n",
    "    # store the losses \n",
    "    log['g_loss'].append(DG_loss)\n",
    "    log['d_loss_real'].append(D_loss[1])\n",
    "    log['d_loss_fake'].append(D_loss[2])\n",
    "    log['d_loss'].append(D_loss)\n",
    "    log['d_loss_gp'].append(D_loss[3])\n",
    "    log['d_loss_wgan'].append(-1 * D_loss[1] + D_loss[2])\n",
    "    \n",
    "    if it%100 == 0:\n",
    "        # every 100 iterations, save the predicted image\n",
    "        sample_images(it, X_test, T_test, G, PAST, PATH)\n",
    "    if it != 0 and it % 50 == 0:\n",
    "        # every 50 iterations, save the training curve\n",
    "        plot_training_curves(log, it, PATH, wgan=True, gp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models' weights\n",
    "G.save_weights(PATH+'G_model.h5')\n",
    "SD.save_weights(PATH+'SD_model.h5')\n",
    "DG.save_weights(PATH+'DG_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the losses\n",
    "np.save(PATH+\"_log\",log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predicted images\n",
    "G_imgs = G.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the difference with predicted image and ground truth image\n",
    "error_images, error_vals, error_means = src.error_distribution(T_test, G_imgs, metric=\"difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 5 images in test dataset with predicted image\n",
    "src.result_plotter(range(5), (X_test[:,:,:,0], T_test[:,:,:,0], G_imgs[:,:,:,0], error_images[:,:,:,0]), save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential prediction for future frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the prediction is True with load_datasets function, it load test dataset and it contains 20 frames\n",
    "past = 2\n",
    "sequence_test = src.load_datasets(prediction=True)\n",
    "#sequence_test = src.augment_data(sequence_test[:100])\n",
    "sequence_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future frames (if past is 2, then 18 future frames will predicted) (ex. past: 3 --> 17)\n",
    "n_next = 5\n",
    "predictions = {}\n",
    "past_frames = sequence_test[...,0:past]\n",
    "test_truth = sequence_test[...,past:past+1]\n",
    "for t in range(n_next):\n",
    "    src.update_output(t)\n",
    "    future = gen.predict(past_frames, batch_size=64)\n",
    "    predictions[f\"{t}\"] = future\n",
    "    past_frames = np.concatenate((past_frames[:,:,:,1:], predictions[f\"{t}\"]), axis=-1)\n",
    "    test_truth = sequence_test[...,past+1+t:past+2+t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted image \n",
    "src.sequence_prediction_plot(name, sequence_test, predictions, past, samples=[44,110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation for scores and correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset to calculate the scores and correlation\n",
    "test_norms = np.load(sys.path[0]+\"/5min_long_pred_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize test samples\n",
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(sequence_test)])\n",
    "renormalized_predictions = np.transpose((np.array([[sample * np.array(test_norms)[i] for i, sample in enumerate(predictions[key])] for key in list(map(str,np.arange(0,n_next)))])[:,:,:,:,0]), (1,2,3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pixel intensities back to dBZ and from there to mm/h. <br>\n",
    "Sources: <br>\n",
    "- https://www.dwd.de/DE/leistungen/radolan/radolan_info/radolan_radvor_op_komposit_format_pdf.pdf?__blob=publicationFile&v=11 (page 10)\n",
    "- <https://web.archive.org/web/20160113151652/http://www.desktopdoppler.com/help/nws-nexrad.htm#rainfall%20rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(0.5**(8/5)*200)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dBZ\n",
    "dBZ_t = renormalized_test*0.5 - 32.5\n",
    "dBZ_p = renormalized_predictions*0.5 - 32.5\n",
    "#mm/h\n",
    "I_t = (0.005*10**(0.1*dBZ_t))**(0.625)\n",
    "I_p = (0.005*10**(0.1*dBZ_p))**(0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_scores = src.get_scores(renormalized_predictions, renormalized_test, n_next, past, thresholds_as_list=[18])\n",
    "scores = src.get_scores(I_p, I_t, n_next, past, thresholds_as_list=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(I_p[10,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(I_t[10,:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(renormalized_predictions[10,:,:,0], vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(renormalized_test[10,:,:,-1], vmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_scores\",scores)\n",
    "np.save(name+\"_intensity_scores\",intensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(scores['pred_1'][\"corr_to_truth\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(intensity_scores['pred_1'][\"corr_to_truth\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(intensity_scores['pred_1'][\"corr_to_input\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
