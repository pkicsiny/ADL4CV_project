{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "from os import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/models')\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17193806076515979355\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1508248780\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1195100970117366386\n",
      "physical_device_desc: \"device: 0, name: GeForce GT 740M, pci bus id: 0000:01:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#forces CPU usage\n",
    "environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, xval, test = src.load_datasets(past_frames=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train, gan_truth, gan_val, gan_val_truth, gan_test, gan_test_truth = src.split_datasets(\n",
    "            train[:2000], xval, test, past_frames=past, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"unet_{past}-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = src.unet(input_shape=(64, 64, past), dropout=0, batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   528         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 16)   4112        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     8224        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 32)     128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     16416       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 32)     128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 2, 2, 64)     32832       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2, 2, 64)     256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 2, 2, 64)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 4, 4, 32)     32800       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4, 4, 32)     128         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 4, 4, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 64)     0           batch_normalization_4[0][0]      \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 8, 8, 32)     32800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 32)     128         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 8, 8, 32)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 64)     0           batch_normalization_3[0][0]      \n",
      "                                                                 leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 16, 16)   16400       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 16)   64          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 32)   0           batch_normalization_2[0][0]      \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 16)   8208        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 1)    513         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 153,857\n",
      "Trainable params: 153,345\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Using L1 loss.***\n",
      "Tensor(\"metrics/relative_error_tensor/truediv:0\", shape=(), dtype=float32)\n",
      "Tensor(\"conv2d_transpose_5_target:0\", shape=(?, ?, ?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss=src.custom_loss(\"l1\"), metrics=[src.relative_error_tensor]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(gan_train,\n",
    "          gan_truth,\n",
    "          batch_size = 64,\n",
    "          epochs=20,\n",
    "          validation_data=(gan_val,gan_val_truth),\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.history\n",
    "hist.history.keys()\n",
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(name+'_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_log\",hist.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(sys.path[1]+f\"/unet_{past}-1/\"+name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future frames. Loads a 20 long sequence with 1000 sequence samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: (1000, 64, 64, 20)\n"
     ]
    }
   ],
   "source": [
    "sequence_test = src.load_datasets(prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_test = src.augment_data(sequence_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 64, 64, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = {}\n",
    "past_frames = sequence_test[...,0:past]\n",
    "test_truth = sequence_test[...,past:past+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(5):\n",
    "    future = model.predict(past_frames, batch_size=64)\n",
    "    predictions[f\"{t}\"] = future\n",
    "    past_frames = np.concatenate((past_frames[:,:,:,1:], predictions[f\"{t}\"]), axis=-1)\n",
    "    test_truth = sequence_test[...,past+1+t:past+2+t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_examples(name, test, predictions_dict, past, samples=0):\n",
    "    fig, axs = plt.subplots(len(samples)*2,past+len(predictions_dict.keys()), figsize=(32, 32))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.0)\n",
    "    for n in range(len(samples)):\n",
    "        vmax = np.max(test[n,:,:,:past])\n",
    "        vmin = 0\n",
    "        print(test.shape)\n",
    "        for i in range(past):\n",
    "            im = axs[2*n,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "        for i in range(past,past+len(predictions_dict.keys())):\n",
    "            im = axs[2*n,i].imshow(predictions_dict[f\"{i-past}\"][samples[n], :,:,0], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Predicted frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Reference frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "    fig.savefig(f\"Plots/{name}_sequence_prediction.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 64, 64, 20)\n"
     ]
    }
   ],
   "source": [
    "save_examples(name, sequence_test, predictions, past, samples=[33]) # 33, 46, 54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.load(sys.path[0]+\"/5min_long_pred_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *4 bc of augmentaion (it concats the frames so the 0th 1000th 2000th and 3000th are the same sample just rotated)\n",
    "test_norms = list(norms)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize test samples\n",
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(sequence_test)])\n",
    "renormalized_predictions = np.transpose((np.array([[sample * np.array(test_norms)[i] for i, sample in enumerate(predictions[key])] for key in ['0', '1', '2', '3', \"4\"]])[:,:,:,:,0]), (1,2,3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pixel intensities back to dBZ and from there to mm/h. <br>\n",
    "Sources: <br>\n",
    "- https://www.dwd.de/DE/leistungen/radolan/radolan_info/radolan_radvor_op_komposit_format_pdf.pdf?__blob=publicationFile&v=11 (page 10)\n",
    "- <https://web.archive.org/web/20160113151652/http://www.desktopdoppler.com/help/nws-nexrad.htm#rainfall%20rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dBZ\n",
    "dBZ_t = renormalized_test*0.5 - 32.5\n",
    "dBZ_p = renormalized_predictions*0.5 - 32.5\n",
    "#mm/h\n",
    "I_t = (0.005*10**(0.1*dBZ_t))**(0.625)\n",
    "I_p = (0.005*10**(0.1*dBZ_p))**(0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkicsiny\\Desktop\\TUM\\3\\ADL4CV\\ADL4CV_project\\src.py:965: RuntimeWarning: invalid value encountered in true_divide\n",
      "  scores[\"csi\"] = hits/(hits+misses+false_alarms)\n",
      "C:\\Users\\pkicsiny\\Desktop\\TUM\\3\\ADL4CV\\ADL4CV_project\\src.py:967: RuntimeWarning: invalid value encountered in true_divide\n",
      "  scores[\"far\"] = false_alarms/(hits+false_alarms)\n",
      "C:\\Users\\pkicsiny\\Desktop\\TUM\\3\\ADL4CV\\ADL4CV_project\\src.py:969: RuntimeWarning: invalid value encountered in true_divide\n",
      "  scores[\"pod\"] = hits/(hits+misses)\n"
     ]
    }
   ],
   "source": [
    "#thresholds: 2, 8, 42\n",
    "thresholds = [0.5]\n",
    "scores = {}\n",
    "for t in range(5): # loop over the predictions\n",
    "    for s in thresholds: # make a dict entry for each threshold score\n",
    "        scores[f\"pred_{t+1}\"] = src.calculate_skill_scores(I_p[...,t:t+1],\n",
    "                                                          I_t[...,past+t:past+1+t],\n",
    "                                                          x=I_t[...,past-1:past],\n",
    "                                                          threshold=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_scores\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scores = np.load(sys.path[1]+\"/\"+name+\"/\"+name+\"_scores.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.477073996073891"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((pd.Series(scores[\"pred_1\"][\"corr_to_input\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5156214354375566"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((pd.Series(scores[\"pred_1\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3305291973648491"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((pd.Series(scores[\"pred_2\"][\"corr_to_input\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2334171819826245"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((pd.Series(scores[\"pred_4\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-62d0ab10cdf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_4_threshold_100\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"corr_to_truth\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_scores' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean((pd.Series(loaded_scores[\"pred_4_threshold_100\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
