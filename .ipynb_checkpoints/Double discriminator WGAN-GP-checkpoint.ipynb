{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the datasets from here: https://drive.google.com/open?id=1mNEblJS0622w5-2mB6yCyMbu7RbcfENL<br>\n",
    "This should contain the following:\n",
    "- __5_min_train__: training data as np array. Shape: (7500, 64, 64, 8)\n",
    "- __5_min_xval__: validation data (currently unused) as np array. Shape: (1500, 64, 64, 8)\n",
    "- __5_min_test__: test data (used as visual validation during training) as np array. Shape: (1000, 64, 64, 8)\n",
    "- __5_min_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence. Shape: (10000,)\n",
    "- __5_min_long_pred__: test data for sequence prediction as np array. We used it for testing after training. Shape: (1000, 64, 64, 20)\n",
    "- __5_min_long_pred_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence for the 5_min_long_pred dataset. Shape: (1000,)\n",
    "- __tgan_1/2/4-1_vx/vy_2000__: optical flow images between the last and second last frames of the input for the first 2000 sequences of the training dataset (__5_min_train__) as np array. The 1/2/4 means the length of the input sequence. We mostly used 2. Shape: (2000, 64, 64, 1)\n",
    "- __germany__: Not needed. (GPS coordinates of Germany. Used for experimenting before.)\n",
    "\n",
    "In the datasets the first axis is stands for the sample, the next two for the frame height and width and the last for the channels which is the time axis here.<br>\n",
    "If data is missing or you cannot acces the drive, please write me an E-Mail: pkicsiny@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5869128162437668326\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1508248780\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1423771880434189164\n",
      "physical_device_desc: \"device: 0, name: GeForce GT 740M, pci bus id: 0000:01:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from functools import partial\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#trainings folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/trainings')\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')\n",
    "#forces CPU usage\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(keras.layers.Concatenate):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms, this outputs a random point on the line\n",
    "    between each pair of input points.\n",
    "    Inheriting from _Merge is a little messy but it was the quickest solution I could think of.\n",
    "    Improvements appreciated.\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((batch_size, 1, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1]) if len(inputs) == 2 else \\\n",
    "               [(weights * inputs[0]) + ((1 - weights) * inputs[1]), (weights * inputs[2]) + ((1 - weights) * inputs[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 2\n",
    "name = f\"tgan_{past}-1\"\n",
    "iterations = 5000\n",
    "batch_size = 16\n",
    "GRADIENT_PENALTY_WEIGHT = 10\n",
    "#generator trainings in 1 iter\n",
    "g = 2\n",
    "#spatial disc trainings in 1 iter\n",
    "s = 1\n",
    "#temporal disc trainings in 1 iter\n",
    "t = 1\n",
    "#initialize random seed\n",
    "RND = 777\n",
    "np.random.seed(RND)\n",
    "#set this to true if you want to use a pretrained model and load its weights from file\n",
    "use_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (7500, 64, 64, 4)\n",
      "Validation data: (1500, 64, 64, 4)\n",
      "Test data: (1000, 64, 64, 4)\n"
     ]
    }
   ],
   "source": [
    "train, xval, test = src.load_datasets(past_frames=past+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to inputs and ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation.\n",
      "Shape of training data:  (8000, 64, 64, 3) \n",
      "Shape of training truth:  (8000, 64, 64, 1) \n",
      "Shape of validation data:  (1500, 64, 64, 3) \n",
      "Shape of validation truth:  (1500, 64, 64, 1) \n",
      "Shape of test data:  (1000, 64, 64, 3) \n",
      "Shape of test truth:  (1000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "gan_train, gan_truth, gan_val, gan_val_truth, gan_test, gan_test_truth = src.split_datasets(\n",
    "            train[:2000], xval, test, past_frames=past+1, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate optical flows between frame t-1 and t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optical flow of the augmented data of the first 2000 training images (8000 images)\n",
    "#vx, vy = src.optical_flow(gan_train[:,:,:,-2:-1], gan_train[:,:,:,-1:], window_size=4, tau=1e-2, init=1) # (n,:,:,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save optical flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed(f\"{name}_vx_2000\",vx) #2000 denotes that they re the flo2 of the first 2000 samples from the training dataset\n",
    "#np.savez_compressed(f\"{name}_vy_2000\",vy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If optical flows are saved, load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx = np.load(sys.path[0]+f\"/{name}_vx_2000.npz\")[\"arr_0\"]\n",
    "vy = np.load(sys.path[0]+f\"/{name}_vy_2000.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess optical flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_optical_flow = src.normalize_flows(vx, vy)\n",
    "flows = np.transpose([[ndimage.median_filter(image[..., ch], 4) for ch in range(2)] for image in normalized_optical_flow], (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make generator but don't compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = src.unet((64, 64, past), dropout=0, batchnorm=True, kernel_size=4, feature_mult=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make discriminators and compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_discriminator = src.spatial_discriminator(condition_shape=(64, 64, past), dropout = 0.25, batchnorm=True, wgan=True)\n",
    "s_discriminator.compile(loss=src.wasserstein_loss,\n",
    "                        optimizer=keras.optimizers.RMSprop(lr=0.00005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_discriminator = src.temporal_discriminator(dropout = 0.25, batchnorm=True, wgan=True)\n",
    "t_discriminator.compile(loss=src.wasserstein_loss,\n",
    "                        optimizer=keras.optimizers.RMSprop(lr=0.00005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and outputs of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_t = keras.layers.Input(shape=(64, 64, past), name='input_condition_')\n",
    "adv = keras.layers.Input(shape=(64, 64, 1), name=\"advected_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generator(frame_t)\n",
    "s_score_fake = s_discriminator([frame_t, generated])\n",
    "t_score_fake = t_discriminator([adv, generated])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze discriminator weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_discriminator.trainable = False\n",
    "t_discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights=[0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.models.Model(inputs=[frame_t, adv], outputs=[generated, s_score_fake, t_score_fake], name=\"combined_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Using L1 loss.***\n"
     ]
    }
   ],
   "source": [
    "combined.compile(loss=[src.custom_loss(loss=\"l1\"),\n",
    "                       src.wasserstein_loss,\n",
    "                       src.wasserstein_loss],\n",
    "                 optimizer=keras.optimizers.Adam(0.0001, 0.5),\n",
    "                 loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in s_discriminator.layers:\n",
    "    l.trainable = True\n",
    "for l in t_discriminator.layers:\n",
    "    l.trainable = True\n",
    "for l in generator.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "s_discriminator.trainable = True\n",
    "t_discriminator.trainable = True\n",
    "generator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_samples = keras.layers.Input(shape=(64, 64, 1), name=\"ground_truth\")\n",
    "adv_real = keras.layers.Input(shape=(64, 64, 1), name=\"real_advected\")\n",
    "\n",
    "frame_t = keras.layers.Input(shape=(64, 64, past), name=\"input_sequence\")\n",
    "adv = keras.layers.Input(shape=(64, 64, 1), name=\"fake_advected\")\n",
    "\n",
    "generated = generator(frame_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_output_generated = s_discriminator([frame_t, generated])\n",
    "ds_output_real = s_discriminator([frame_t, real_samples])\n",
    "s_averaged_samples = RandomWeightedAverage()([real_samples, generated])\n",
    "ds_output_avg = s_discriminator([frame_t, s_averaged_samples])\n",
    "\n",
    "dt_output_generated = t_discriminator([adv, generated])\n",
    "dt_output_real = t_discriminator([adv_real, real_samples])\n",
    "t_averaged_samples, t_averaged_advections = RandomWeightedAverage()([real_samples, generated, adv_real, adv])\n",
    "dt_output_avg = t_discriminator([t_averaged_advections, t_averaged_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_partial_gp_loss = partial(src.gradient_penalty_loss,\n",
    "                          averaged_samples=s_averaged_samples,\n",
    "                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "s_partial_gp_loss.__name__ = 's_gradient_penalty' \n",
    "\n",
    "t_partial_gp_loss = partial(src.gradient_penalty_loss,\n",
    "                          averaged_samples=t_averaged_samples,\n",
    "                          gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "t_partial_gp_loss.__name__ = 't_gradient_penalty' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = keras.models.Model(inputs=[frame_t, real_samples],\n",
    "                                   outputs=[ds_output_real,\n",
    "                                            ds_output_generated,\n",
    "                                            ds_output_avg])\n",
    "\n",
    "Dt = keras.models.Model(inputs=[frame_t, real_samples, adv, adv_real],\n",
    "                                   outputs=[dt_output_real,\n",
    "                                            dt_output_generated,\n",
    "                                            dt_output_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds.compile(optimizer=keras.optimizers.RMSprop(lr=0.00005),\n",
    "          loss=[src.wasserstein_loss, src.wasserstein_loss, s_partial_gp_loss])\n",
    "\n",
    "Dt.compile(optimizer=keras.optimizers.RMSprop(lr=0.00005),\n",
    "          loss=[src.wasserstein_loss, src.wasserstein_loss, t_partial_gp_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_y = np.ones((batch_size, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros((batch_size, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\"g_loss\":[],\n",
    "       \"ds_loss\":[],\n",
    "       \"dt_loss\":[],\n",
    "       \"ds_loss_real\":[],\n",
    "       \"ds_loss_fake\":[],\n",
    "       \"ds_loss_avg\":[],\n",
    "       'ds_loss_wgan':[],\n",
    "       \"dt_loss_real\":[],\n",
    "       \"dt_loss_fake\":[],\n",
    "       \"dt_loss_avg\":[],\n",
    "       'dt_loss_wgan':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for it in range(iterations):\n",
    "#batch\n",
    "#idx = range(it%nb_batches * batch_size,(it%nb_batches + 1) * batch_size)\n",
    "    if (it % 1000) < 25 or it % 500 == 0: # 25 times in 1000, every 500th\n",
    "        d_iters = 10\n",
    "    else:\n",
    "        d_iters = 10\n",
    "        \n",
    "    s_discriminator.trainable = True\n",
    "    for l in s_discriminator.layers: l.trainable = True\n",
    "    t_discriminator.trainable = True\n",
    "    for l in t_discriminator.layers: l.trainable = True\n",
    "    generator.trainable = False\n",
    "    for l in generator.layers: l.trainable = False\n",
    "            \n",
    "    for d_it in range(d_iters):\n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        \n",
    "        ds_loss = Ds.train_on_batch([training_batch, real_imgs], [negative_y, positive_y, dummy_y])\n",
    "    \n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        aux_batch = gan_train[idx,:,:,:-1]\n",
    "            \n",
    "        advected_aux_gen = generator.predict(aux_batch)\n",
    "        advected_aux_truth = training_batch[:,:,:,-1:]\n",
    "        for i in range(10):\n",
    "            advected_aux_gen = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected_aux_gen, -flows[idx]), axis=-1)])\n",
    "            advected_aux_truth = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected_aux_truth, -flows[idx]), axis=-1)])\n",
    "        \n",
    "        dt_loss = Dt.train_on_batch([training_batch,\n",
    "                                     real_imgs,\n",
    "                                     advected_aux_gen,\n",
    "                                     advected_aux_truth], [negative_y, positive_y, dummy_y])\n",
    "        \n",
    "        print(f\"{it}/{d_it} [Ds loss real: {ds_loss[1]} Ds loss fake: {ds_loss[2]} Ds loss avg: {ds_loss[3]}] \\n\"+\n",
    "              f\"{it}/{d_it} [Dt loss real: {dt_loss[1]} Dt loss fake: {dt_loss[2]} Dt loss avg: {dt_loss[3]}]\")\n",
    "\n",
    "#Generator\n",
    "    s_discriminator.trainable = False\n",
    "    for l in s_discriminator.layers: l.trainable = False\n",
    "    t_discriminator.trainable = False\n",
    "    for l in t_discriminator.layers: l.trainable = False\n",
    "    generator.trainable = True\n",
    "    for l in generator.layers: l.trainable = True\n",
    "        \n",
    "    for tg in range(g):\n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        aux_batch = gan_train[idx,:,:,:-1]\n",
    "\n",
    "        advected = generator.predict(aux_batch)\n",
    "        for i in range(10):\n",
    "            advected = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected, -flows[idx]), axis=-1)])  \n",
    "    \n",
    "        g_loss = combined.train_on_batch([training_batch, advected], [real_imgs, negative_y, negative_y])\n",
    "    \n",
    "    log[\"g_loss\"].append(g_loss)\n",
    "    log[\"ds_loss\"].append(ds_loss) \n",
    "    log[\"dt_loss\"].append(dt_loss)\n",
    "    log[\"ds_loss_real\"].append(ds_loss[1])\n",
    "    log[\"ds_loss_fake\"].append(ds_loss[2])\n",
    "    log[\"ds_loss_avg\"].append(ds_loss[3])\n",
    "    log['ds_loss_wgan'].append(-1 * ds_loss[1] + ds_loss[2])\n",
    "    log[\"dt_loss_real\"].append(dt_loss[1])\n",
    "    log[\"dt_loss_fake\"].append(dt_loss[2])\n",
    "    log[\"dt_loss_avg\"].append(dt_loss[3])\n",
    "    log['dt_loss_wgan'].append(-1 * dt_loss[1] + dt_loss[2])\n",
    "    \n",
    "    \n",
    "    print(f\"\\033[1m {it} [G loss: {g_loss}]\\033[0m \\n\"+\n",
    "          f\" Ds: real loss: {ds_loss[1]}, fake loss: {ds_loss[2]}, avg loss: {ds_loss[3]} \\n\"+\n",
    "          f\" Dt: real loss: {dt_loss[1]}, fake loss: {dt_loss[2]}, avg loss: {dt_loss[3]}\")\n",
    "    if it%100 == 0 and it>0:\n",
    "        src.sample_images(it, gan_test[...,1:], gan_test_truth, past, generator)\n",
    "        src.plot_advections(advected_aux_gen, advected_aux_truth, it)\n",
    "        src.plot_temporal_training_curves(log, it, name, wgan=True)\n",
    "        src.update_output(\"\")\n",
    "\n",
    "src.sample_images(iterations, gan_test[...,1:], gan_test_truth, past, generator)\n",
    "src.plot_temporal_training_curves(log, iterations, name, wgan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log[\"dt_loss_real\"],label=\"real\")\n",
    "plt.plot(log[\"dt_loss_fake\"],label=\"fake\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.plot_temporal_training_curves(log, iterations, name, wgan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_g_loss = np.array(log[\"g_loss\"])[:,0]\n",
    "total_d_loss = np.array(log[\"d_loss\"])[:,0]\n",
    "smoothed_tgl = src.smooth(np.array(log[\"g_loss\"])[:,0])\n",
    "smoothed_tdl = src.smooth(np.array(log[\"d_loss\"])[:,0])\n",
    "objective_loss = np.array(log[\"g_loss\"])[:,1]\n",
    "\n",
    "# plot 'em\n",
    "f, (a0, a1) = plt.subplots(2,1, gridspec_kw = {'height_ratios':[5, 2]})\n",
    "a0.plot(total_g_loss, alpha=0.3, c=\"b\")\n",
    "a0.plot(total_d_loss, alpha=0.3, c=\"orange\")\n",
    "a0.plot(smoothed_tgl, c=\"b\", label=\"generator\")\n",
    "a0.grid()\n",
    "a0.plot(smoothed_tdl, c=\"orange\", label=\"discriminator\")\n",
    "a0.legend()\n",
    "a1.plot(objective_loss, alpha=0.9, c=\"green\", label=\"L1 objective\")\n",
    "a1.grid()\n",
    "a1.legend()\n",
    "f.text(0.5, 0, 'Iterations', ha='center', va='center')\n",
    "f.text(0, 0.5, 'Loss', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(name+'_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_log\",log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.save_weights(name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined.load_weights(sys.path[1]+\"/\"+name+\"/\"+name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future frames. Loads a 20 long sequence with 1000 sequence samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_test = src.load_datasets(prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_test = src.augment_data(sequence_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = combined.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = {}\n",
    "past_frames = sequence_test[...,0:past]\n",
    "test_truth = sequence_test[...,past:past+1]\n",
    "for t in range(5):\n",
    "    future = generator.predict(past_frames, batch_size=64)\n",
    "    predictions[f\"{t}\"] = future\n",
    "    past_frames = np.concatenate((past_frames[:,:,:,1:], predictions[f\"{t}\"]), axis=-1)\n",
    "    test_truth = sequence_test[...,past+1+t:past+2+t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_examples(name, test, predictions_dict, past, samples=0):\n",
    "    fig, axs = plt.subplots(len(samples)*2,past+len(predictions_dict.keys()), figsize=(32, 32))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.0)\n",
    "    for n in range(len(samples)):\n",
    "        vmax = np.max(test[n,:,:,:past])\n",
    "        vmin = 0\n",
    "        print(test.shape)\n",
    "        for i in range(past):\n",
    "            im = axs[2*n,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "        for i in range(past,past+len(predictions_dict.keys())):\n",
    "            im = axs[2*n,i].imshow(predictions_dict[f\"{i-past}\"][samples[n], :,:,0], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Predicted frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Reference frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "    fig.savefig(f\"Plots/{name}_sequence_prediction.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_examples(name, sequence_test, predictions, past, samples=[33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renormalize intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.load(sys.path[0]+\"/5min_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *4 bc of augmentaion (it concats the frames so the 0th 1000th 2000th and 3000th are the same sample just rotated)\n",
    "test_norms = list(norms[9000:])*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize test samples\n",
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(sequence_test)])\n",
    "renormalized_predictions = np.transpose((np.array([[sample * np.array(test_norms)[i] for i, sample in enumerate(predictions[key])] for key in ['0', '1', '2', '3', \"4\"]])[:,:,:,:,0]), (1,2,3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pixel intensities back to dBZ and from there to mm/h. <br>\n",
    "Sources: <br>\n",
    "- https://www.dwd.de/DE/leistungen/radolan/radolan_info/radolan_radvor_op_komposit_format_pdf.pdf?__blob=publicationFile&v=11 (page 10)\n",
    "- <https://web.archive.org/web/20160113151652/http://www.desktopdoppler.com/help/nws-nexrad.htm#rainfall%20rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dBZ\n",
    "dBZ_t = renormalized_test*0.5 - 32.5\n",
    "dBZ_p = renormalized_predictions*0.5 - 32.5\n",
    "#mm/h\n",
    "I_t = (0.005*10**(0.1*dBZ_t))**(0.625)\n",
    "I_p = (0.005*10**(0.1*dBZ_p))**(0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds: 2, 8, 42\n",
    "thresholds = [0.5]\n",
    "scores = {}\n",
    "for t in range(5): # loop over the predictions\n",
    "    for s in thresholds: # make a dict entry for each threshold score\n",
    "        scores[f\"pred_{t+1}\"] = src.calculate_skill_scores(I_p[...,t:t+1],\n",
    "                                                          I_t[...,past+t:past+1+t],\n",
    "                                                          x=I_t[...,past-1:past],\n",
    "                                                          threshold=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_scores\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"pred_1\"][\"corr_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scores = np.load(sys.path[1]+\"/\"+name\"/\"+name+\"_scores.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loaded_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_1\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_1\"][\"corr_to_input\"]).dropna()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
