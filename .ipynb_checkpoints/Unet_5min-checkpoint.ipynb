{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get rain data and visualise for overview <font color='orange'>&#9728;</font>\n",
    "- cut and split data to reasonable train, val, test sets <font color='orange'>&#9728;</font>\n",
    "- set up first generator and discriminator and overfit, play around with it, get first results (_November/December_)\n",
    "- train on small training set and improve losses, accuracy, results (_December_)\n",
    "- experiment with temporal discriminator (_December/January_)\n",
    "- experiment with wind data (_January_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forces CPU usage\n",
    "environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________-\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain measurements\n",
    "Measurements are downloaded from the DWD (German weather service) open data server: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/__<br>\n",
    "I'm working with the data of August 2010 (based on [this](https://tradingeconomics.com/germany/precipitation)), so I have downloaded this: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/2010/RW-201008.tar__<br>\n",
    "_DWD manual:_<br>\n",
    "__ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/BESCHREIBUNG_gridsgermany_hourly_radolan_historical_asc_de.pdf__<br><br>\n",
    "This contains radar maps recorded in every hour. Each map has a resolution of $900\\times900$ pixels and each pixel corresponds to an $1\\,km\\times1\\,km$ area in reality. Pixel values are the precipitation height in $0.1\\,mm$.\n",
    "Below I'm importing the data of this as a series of numpy arrays and plot them to see the acual radar map. The _sys.path[0]_ is the path on my computer and it can be different for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(sys.path[0]+\"/5_minute.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_res_train = images[\"low_res_train\"]\n",
    "#low_res_xval = images[\"low_res_xval\"]\n",
    "#low_res_test = images[\"low_res_test\"]\n",
    "#overfit = np.reshape(images['images'],np.shape(images['images'])+(1,))\n",
    "train = np.reshape(images[\"train\"],np.shape(images[\"train\"])+(1,))\n",
    "xval = np.reshape(images[\"xval\"],np.shape(images[\"xval\"])+(1,))\n",
    "test = np.reshape(images[\"test\"],np.shape(images[\"test\"])+(1,))\n",
    "print(f\"Training data: {train.shape}\\nValidation data: {xval.shape}\\nTest data: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_train     = np.reshape(train[:,0,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "unet_truth     = np.reshape(train[:,1,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "unet_val       = np.reshape(xval[:,0,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "unet_val_truth = np.reshape(xval[:,1,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "print(unet_train.shape,\"\\n\",unet_truth.shape,\"\\n\",unet_val.shape,\"\\n\",unet_truth.shape)\n",
    "unet_test      = np.reshape(test[:,0,:,:,:],((test.shape[0],)+test.shape[2:]))\n",
    "unet_test_truth = np.reshape(test[:,1,:,:,:],((test.shape[0],)+test.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_train = np.reshape(train[5,0,:,:,:],((1,)+train.shape[2:]))\n",
    "overfit_truth = np.reshape(train[5,1,:,:,:],((1,)+train.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "src.visualise_data(xval[:,:,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2=keras.Sequential()\n",
    "\n",
    "init = keras.layers.Input(shape=(64,64,1))\n",
    "ConvDown1  = keras.layers.Conv2D(filters=8,kernel_size=(2,2),strides=(1,1),padding=\"same\")(init)\n",
    "Lr1 = keras.layers.LeakyReLU(alpha=0.0)(ConvDown1)\n",
    "#64\n",
    "ConvDown2  = keras.layers.Conv2D(filters=16,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr1)\n",
    "Lr2 = keras.layers.LeakyReLU(alpha=0.0)(ConvDown2)\n",
    "#32\n",
    "ConvDown3  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr2)\n",
    "Lr3 = keras.layers.LeakyReLU(alpha=0.0)(ConvDown3)\n",
    "#16\n",
    "ConvDown4  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr3)\n",
    "Lr4 = keras.layers.LeakyReLU(alpha=0.0)(ConvDown4)\n",
    "#8\n",
    "ConvDown5  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr4)\n",
    "Lr5 = keras.layers.LeakyReLU(alpha=0.0)(ConvDown5)\n",
    "#4\n",
    "\n",
    "UpSamp1 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr5)\n",
    "#8\n",
    "merge1  = keras.layers.concatenate([ConvDown4,UpSamp1],axis=-1)#(UpSamp1)\n",
    "Conv1   = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(1,1),padding=\"same\")(merge1)\n",
    "Lr6 = keras.layers.LeakyReLU(alpha=0.0)(Conv1)\n",
    "#8\n",
    "UpSamp2 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr6)\n",
    "#16\n",
    "merge2  = keras.layers.concatenate([ConvDown3,UpSamp2],axis=-1)#(UpSamp2)\n",
    "Conv2   = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(1,1),padding=\"same\")(merge2)\n",
    "Lr7  = keras.layers.LeakyReLU(alpha=0.0)(Conv2)\n",
    "#16\n",
    "UpSamp3 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr7)\n",
    "\n",
    "#32\n",
    "Conv3   = keras.layers.Conv2D(filters=16,kernel_size=(4,4),strides=(1,1),padding=\"same\")(UpSamp3)\n",
    "Lr8  = keras.layers.LeakyReLU(alpha=0.0)(Conv3)\n",
    "\n",
    "UpSamp4 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr8)\n",
    "#64\n",
    "Conv4   = keras.layers.Conv2D(filters=8,kernel_size=(4,4),strides=(1,1),padding=\"same\",activation = 'relu')(UpSamp4)\n",
    "\n",
    "Conv5   = keras.layers.Conv2D(filters=1,kernel_size=(4,4),strides=(1,1),padding=\"same\",activation = 'elu')(Conv4)\n",
    "\n",
    "model2 = keras.models.Model(inputs=init, outputs=Conv5)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model2.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss=src.custom_loss([\"l2\", \"l1\", \"l33\"]), metrics=[src.relative_error_tensor]) \n",
    "model2.fit(unet_train[:10],\n",
    "          unet_truth[:10],\n",
    "          batch_size = 10,\n",
    "          epochs=2,\n",
    "          validation_data=(unet_val[:10],unet_val_truth[:10]),\n",
    "          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model2.history\n",
    "hist.history.keys()\n",
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig('training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = model2.predict(unet_test, batch_size=100)\n",
    "truth       = unet_test_truth\n",
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = src.arg_getter(truth, predictions)\n",
    "args[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images, error_vals, error_means = src.error_distribution(truth,predictions,metric=\"relative_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_plotter(args[0:2], (unet_test[:,:,:,0], truth[:,:,:,0], predictions[:,:,:,0], error_images[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
