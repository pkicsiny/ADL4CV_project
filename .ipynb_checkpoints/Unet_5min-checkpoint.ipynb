{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "from os import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14751315593167467993\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1508248780\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2946742521461545970\n",
      "physical_device_desc: \"device: 0, name: GeForce GT 740M, pci bus id: 0000:01:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#forces CPU usage\n",
    "environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (7500, 64, 64, 3)\n",
      "Validation data: (1500, 64, 64, 3)\n",
      "Test data: (1000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "train, xval, test = src.load_datasets(past_frames=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation.\n",
      "Shape of training data:  (8000, 64, 64, 2) \n",
      "Shape of training truth:  (8000, 64, 64, 1) \n",
      "Shape of validation data:  (1500, 64, 64, 2) \n",
      "Shape of validation truth:  (1500, 64, 64, 1) \n",
      "Shape of test data:  (1000, 64, 64, 2) \n",
      "Shape of test truth:  (1000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "gan_train, gan_truth, gan_val, gan_val_truth, gan_test, gan_test_truth = src.split_datasets(\n",
    "            train[:2000], xval, test, past_frames=2, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unet2(input_shape=(64, 64, 1), dropout=0.0, batchnorm=False):\n",
    "    lr = 0.0\n",
    "    \n",
    "    init = keras.layers.Input(shape=input_shape)\n",
    "    ConvDown1  = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(2,2),padding=\"same\")(init)\n",
    "    if batchnorm:\n",
    "        ConvDown1 = keras.layers.BatchNormalization()(ConvDown1)\n",
    "    Lr1 = keras.layers.LeakyReLU(alpha=lr)(ConvDown1)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr1 = keras.layers.Dropout(dropout)(Lr1)\n",
    "    #32\n",
    "    ConvDown2  = keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(Lr1)\n",
    "    if batchnorm:\n",
    "        ConvDown2 = keras.layers.BatchNormalization()(ConvDown2)\n",
    "    Lr2 = keras.layers.LeakyReLU(alpha=lr)(ConvDown2)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr2 = keras.layers.Dropout(dropout)(Lr2)\n",
    "    #16\n",
    "    ConvDown3  = keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(Lr2)\n",
    "    if batchnorm:\n",
    "        ConvDown3 = keras.layers.BatchNormalization()(ConvDown3)\n",
    "    Lr3 = keras.layers.LeakyReLU(alpha=lr)(ConvDown3)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr3 = keras.layers.Dropout(dropout)(Lr3)\n",
    "    #8\n",
    "    ConvDown4  = keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(Lr3)\n",
    "    if batchnorm:\n",
    "        ConvDown4 = keras.layers.BatchNormalization()(ConvDown4)\n",
    "    Lr4 = keras.layers.LeakyReLU(alpha=lr)(ConvDown4)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr4 = keras.layers.Dropout(dropout)(Lr4)\n",
    "    #4\n",
    "    ConvDown5  = keras.layers.Conv2D(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(Lr4)\n",
    "    if batchnorm:\n",
    "        ConvDown5 = keras.layers.BatchNormalization()(ConvDown5)\n",
    "    Lr5 = keras.layers.LeakyReLU(alpha=lr)(ConvDown5)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr5 = keras.layers.Dropout(dropout)(Lr5)\n",
    "    #2\n",
    "    \n",
    "    UpConv1 = keras.layers.Conv2DTranspose(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(Lr5)\n",
    "    if batchnorm:\n",
    "        UpConv1 = keras.layers.BatchNormalization()(UpConv1)\n",
    "    Lr6 = keras.layers.LeakyReLU(alpha=lr)(UpConv1)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr6 = keras.layers.Dropout(dropout)(Lr6)\n",
    "    merge1  = keras.layers.concatenate([ConvDown4,Lr6],axis=-1)\n",
    "    #4\n",
    "    UpConv2 = keras.layers.Conv2DTranspose(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(merge1)\n",
    "    if batchnorm:\n",
    "        UpConv2 = keras.layers.BatchNormalization()(UpConv2)\n",
    "    Lr7  = keras.layers.LeakyReLU(alpha=lr)(UpConv2)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr7 = keras.layers.Dropout(dropout)(Lr7)\n",
    "    merge2  = keras.layers.concatenate([ConvDown3,Lr7],axis=-1)\n",
    "    #8\n",
    "    UpConv3 = keras.layers.Conv2DTranspose(filters=64,kernel_size=(4,4),strides=(2,2),padding=\"same\")(merge2)\n",
    "    if batchnorm:\n",
    "        UpConv3 = keras.layers.BatchNormalization()(UpConv3)\n",
    "    Lr8  = keras.layers.LeakyReLU(alpha=lr)(UpConv3)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr8 = keras.layers.Dropout(dropout)(Lr8)\n",
    "    merge3  = keras.layers.concatenate([ConvDown2,Lr8],axis=-1)\n",
    "    #16\n",
    "    UpConv4 = keras.layers.Conv2DTranspose(filters=32,kernel_size=(4,4),strides=(2,2),padding=\"same\")(merge3)\n",
    "    if batchnorm:\n",
    "        UpConv4 = keras.layers.BatchNormalization()(UpConv4)\n",
    "    Lr9  = keras.layers.LeakyReLU(alpha=lr)(UpConv4)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr9 = keras.layers.Dropout(dropout)(Lr9)\n",
    "    merge4  = keras.layers.concatenate([ConvDown1,Lr9],axis=-1)\n",
    "    #32\n",
    "    UpConv5 = keras.layers.Conv2DTranspose(filters=1,kernel_size=(4,4),strides=(2,2),padding=\"same\")(merge4)\n",
    "    if batchnorm:\n",
    "        UpConv5 = keras.layers.BatchNormalization()(UpConv5)\n",
    "    Lr10  = keras.layers.LeakyReLU(alpha=lr)(UpConv5)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        Lr10 = keras.layers.Dropout(dropout)(Lr10)\n",
    "    #64\n",
    "    \n",
    "    return keras.models.Model(inputs=init, outputs=Lr10)\n",
    "unet2().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = src.unet(input_shape=(64, 64, 2), dropout=0, batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 2)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 16)   144         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 64, 64, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   1040        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   2080        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 32)     4128        leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 32)     128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 8, 8, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 64)     8256        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 4, 4, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 4, 4, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 8, 8, 64)     0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 96)     0           batch_normalization_13[0][0]     \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 32)     49184       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 32)     128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 8, 8, 32)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 16, 16, 32)   0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   32800       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 32, 32, 32)   0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 48)   0           batch_normalization_11[0][0]     \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 16)   12304       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 64, 64, 16)   0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 64, 64, 32)   0           batch_normalization_10[0][0]     \n",
      "                                                                 up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 16)   8208        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 16)   64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 64, 64, 16)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 1)    257         leaky_re_lu_18[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 119,425\n",
      "Trainable params: 118,913\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Using L1 loss.***\n",
      "Tensor(\"metrics_1/relative_error_tensor/truediv:0\", shape=(), dtype=float32)\n",
      "Tensor(\"conv2d_20_target:0\", shape=(?, ?, ?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss=src.custom_loss(\"l1\"), metrics=[src.relative_error_tensor]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1500 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(gan_train,\n",
    "          gan_truth,\n",
    "          batch_size = 64,\n",
    "          epochs=20,\n",
    "          validation_data=(gan_val,gan_val_truth),\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"unet_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.history\n",
    "hist.history.keys()\n",
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig(name+'_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(name+\"_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = keras.models.load_model(\"C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/trainings/U-net_1-1/\"+name+\"_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded.complie(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0002),loss=src.custom_loss(\"l1\"), metrics=[src.relative_error_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = src.augment_data(test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = augmented[...,0:1]\n",
    "test_truth = augmented[...,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = reloaded.predict(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = src.arg_getter(test_truth, predictions)\n",
    "args[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images, error_vals, error_means = src.error_distribution(test_truth,predictions,metric=\"relative_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src.result_plotter(args[15:20], (test_data[:,:,:,0], test_truth[:,:,:,0], predictions[:,:,:,0], error_images[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize\n",
    "norms = np.load(sys.path[0]+\"/5min_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norms = list(norms[9000:])*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(test_data)])\n",
    "renormalized_truth = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(test_truth)])\n",
    "renormalized_predictions = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds: 10, \n",
    "scores = src.calculate_skill_scores(renormalized_predictions, renormalized_truth, x=renormalized_test, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"training_curve\"] = hist.history['loss']\n",
    "scores[\"validation_curve\"] = hist.history['val_loss']\n",
    "scores[\"training_metric\"] = hist.history['relative_error_tensor']\n",
    "scores[\"validation_metric\"] = hist.history['val_relative_error_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(name+\"_scores\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scores = np.load(\"C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/trainings/U-net_1-1//\"+name+\"_scores.npz\",scores)[\"arr_0\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"far\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"csi\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pod\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.load(sys.path[0]+\"/5min_norms_compressed.npz\")[\"arr_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
