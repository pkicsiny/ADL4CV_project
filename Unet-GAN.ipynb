{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9308214746436472431\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "from os import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')\n",
    "#forces CPU usage\n",
    "environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________-\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain measurements\n",
    "Measurements are downloaded from the DWD (German weather service) open data server: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/__<br>\n",
    "I'm working with the data of August 2010 (based on [this](https://tradingeconomics.com/germany/precipitation)), so I have downloaded this: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/2010/RW-201008.tar__<br>\n",
    "_DWD manual:_<br>\n",
    "__ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/BESCHREIBUNG_gridsgermany_hourly_radolan_historical_asc_de.pdf__<br><br>\n",
    "This contains radar maps recorded in every hour. Each map has a resolution of $900\\times900$ pixels and each pixel corresponds to an $1\\,km\\times1\\,km$ area in reality. Pixel values are the precipitation height in $0.1\\,mm$.\n",
    "Below I'm importing the data of this as a series of numpy arrays and plot them to see the acual radar map. The _sys.path[0]_ is the path on my computer and it can be different for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the paths are probably different for you\n",
    "w_dir = Dataset(sys.path[0]+'/wind_direction/DD_201008_CF.nc')  # direction DD\n",
    "w_vel = Dataset(sys.path[0]+'/wind_speed/FF_201008_CF.nc')  # velocity FF\n",
    "rho = Dataset(sys.path[0]+\"/rain_density/rho.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n",
      "(4, 64, 64, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0474dac64f62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_tempoGAN_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"field\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DD\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-daa34bd5386a>\u001b[0m in \u001b[0;36mgenerate_tempoGAN_datasets\u001b[1;34m(rain_density, wind_dir, n, length, size, split, normalize)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrain_density\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwind_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwind_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py\u001b[0m in \u001b[0;36m__array_finalize__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   2997\u001b[0m                     \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"K\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2999\u001b[1;33m                 \u001b[0m_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_mask_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3000\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3001\u001b[0m                 \u001b[1;31m# Take a view so shape changes, etc., do not propagate back.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = generate_tempoGAN_datasets(rho[\"field\"], w_dir[\"DD\"], n=10, length=4, size=64, split=(6,2,2), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[\"train\"]\n",
    "xval = data[\"xval\"]\n",
    "test = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d4e3a99be0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py:897: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:718: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d4e3a192e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train[5,3,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tempoGAN_datasets(rain_density, wind_dir, n=10, length=2, size=64, split=None, normalize=False):\n",
    "    \"\"\"\n",
    "    Splits input array into training, cross validation and test sets.\n",
    "    :param rain_density: .nc file containing interpolated rain maps on wind grid. A masked np array of shape\n",
    "     744*938*720.\n",
    "    :param wind_dir: same as the rain but these are the wind maps on the same grid. They have the same shape. The\n",
    "    vx and vy channels are calculated from this.\n",
    "    :param n: int, total number of data instances to make\n",
    "    :param length: int, number of consecutive frames on time axis to cut. It cuts length frames from all three channels.\n",
    "    (rho, vx, vy)\n",
    "    :param size: int, height and width in pixels of each frame\n",
    "    :param split: list or np array of either floats between 0 and 1 or positive integers. Set to None by deafult\n",
    "    which means no splitting, just return all instances in one set.\n",
    "    :param normalize: boolean, if true, the rain maps will be normalized between [0,1]. They are simply divided by the\n",
    "    max pixel value in the series. (Wind is always between [-1,1])\n",
    "    :return: 3D np array, either one dataset of smaller image frames or three datasets for training,\n",
    "    cross validating and testing\n",
    "    \"\"\"\n",
    "    time = 743\n",
    "    h = rain_density[0].shape[0]  # 938\n",
    "    w = rain_density[0].shape[1]  # 720\n",
    "\n",
    "    images = np.zeros(\n",
    "        (n, length, size, size, 3))  # n series, each of size size**2 and rho,vx,vy,future frames as channels\n",
    "    for i in range(n):\n",
    "        #src.update_output(f\"[{i+1}/{n}]\")\n",
    "        # draw 3 random numbers for map number and idx of top left pixel of window\n",
    "        valid = 0\n",
    "        while not valid:\n",
    "            anchor = (np.random.randint(0, time - 2), np.random.randint(0, h - size), np.random.randint(0, w - size))\n",
    "            image = np.empty((length, size, size, 3))\n",
    "            for j in range(length):\n",
    "                r = rain_density[anchor[0] + j]\n",
    "                x = -np.flip(np.sin(np.deg2rad(wind_dir[anchor[0] + 1 + j])), axis=0)\n",
    "                y = -np.flip(np.cos(np.deg2rad(wind_dir[anchor[0] + 1 + j])), axis=0)\n",
    "                image[j, :, :, 0] = r[anchor[1]:anchor[1] + size, anchor[2]:anchor[2] + size].filled(np.nan)\n",
    "                image[j, :, :, 1] = x[anchor[1]:anchor[1] + size, anchor[2]:anchor[2] + size].filled(np.nan)\n",
    "                image[j, :, :, 2] = y[anchor[1]:anchor[1] + size, anchor[2]:anchor[2] + size].filled(np.nan)\n",
    "            # first channel is the current frame, the next two are the wind x and y components where the index is shifted by 1\n",
    "            # because the rain dates are in xx:50 resolution but the wind is in xx:00. The next channels are the next rain fields\n",
    "            # to be predicted\n",
    "            valid = valid_image(image)\n",
    "        images[i] += image\n",
    "\n",
    "    if normalize:  # to [0,1] only for rain\n",
    "        images[:, :, :, :, 0] = np.array([s[:, :, :, 0] / s[:, :, :, 0].max() for s in images])\n",
    "    txt = f\"Shape of data: {np.shape(images)}\"\n",
    "    if split is not None:  # split\n",
    "        if all((r <= 1) & (r >= 0) for r in split):\n",
    "            assert (sum(split) == 1), \"Split values must sum up to 1.\"\n",
    "            train = images[:int(n * split[0])]\n",
    "            xval = images[int(n * split[0]):int(n * (split[0] + split[1]))]\n",
    "            test = images[int(n * (split[0] + split[1])):]\n",
    "        elif all(isinstance(r, int) for r in split):\n",
    "            train = images[:int(n * split[0] / sum(split))]\n",
    "            xval = images[int(n * split[0] / sum(split)):int(n * (split[0] + split[1]) / sum(split))]\n",
    "            test = images[int(n * (split[0] + split[1]) / sum(split)):]\n",
    "        else:\n",
    "            sys.exit(\"All split values must be either fractions for percentages or integers.\")\n",
    "\n",
    "        txt = txt + f\"\\n\\nTraining set: {np.shape(train)}\\nValidation set: {np.shape(xval)}\\nTest set: {np.shape(test)}\"\n",
    "        #src.update_output(txt)\n",
    "        return {\"train\": train,\n",
    "                \"xval\": xval,\n",
    "                \"test\": test, }\n",
    "    else:  # no split\n",
    "       # src.update_output(txt)\n",
    "        return images\n",
    "\n",
    "def valid_image(image):\n",
    "    \"\"\"\n",
    "    Filters out some useless data. In the junk variable several conditions are defined to check on the images.\n",
    "    Currently it checks the number of different entry values and if 0s or 1s make up 0.75 part of the whole data.\n",
    "    This throws out the cuts made inside or almost inside the mask region and rainless areas.\n",
    "    Still can be improved.\n",
    "    :param image: 3D np array, dimensions are the number of consecutive frames, height and width\n",
    "    :return: bool, whether the data instance is valid in terms of usability\n",
    "    \"\"\"\n",
    "    if len(np.shape(image)) == 3:  # one channel frames\n",
    "        junk = [len(set(np.array(frame).flatten()[\n",
    "                            ~np.isnan(np.array(frame).flatten())])) <= 8 for frame in image]\n",
    "    else:  # three channel frames\n",
    "        # frame is a triplet for tempogan images\n",
    "        # only rain channel\n",
    "        print(np.shape(image))\n",
    "        junk = [len(set(np.array(frame[:, :, :, 0]).flatten()[\n",
    "                            ~np.isnan(np.array(frame[:, :, :, 0]).flatten())])) <= 12 for frame in image]\n",
    "\n",
    "    junk += [len(np.array(frame).flatten()[\n",
    "                     np.isnan(np.array(frame).flatten())]) > 0.25 * len(np.array(frame).flatten()) for frame in image]\n",
    "    return 0 if any(junk) else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, xval, test = src.load_datasets(\"5min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_train     = np.reshape(train[:,0,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "unet_truth     = np.reshape(train[:,1,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "unet_val       = np.reshape(xval[:,0,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "unet_val_truth = np.reshape(xval[:,1,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "print(unet_train.shape,\"\\n\",unet_truth.shape,\"\\n\",unet_val.shape,\"\\n\",unet_truth.shape)\n",
    "unet_test      = np.reshape(test[:,0,:,:,:],((test.shape[0],)+test.shape[2:]))\n",
    "unet_test_truth = np.reshape(test[:,1,:,:,:],((test.shape[0],)+test.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(unet_test[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_train = np.reshape(train[5,0,:,:,:],((1,)+train.shape[2:]))\n",
    "overfit_truth = np.reshape(train[5,1,:,:,:],((1,)+train.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "src.visualise_data(xval[:,:,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model2=keras.Sequential()\n",
    "def unet():\n",
    "    init       = keras.layers.Input(shape=(64,64,1))\n",
    "    ConvDown1  = keras.layers.Conv2D(filters=8,kernel_size=(2,2),strides=(1,1),padding=\"same\")(init)\n",
    "    Lr1        = keras.layers.LeakyReLU(alpha=0.1)(ConvDown1)\n",
    "    #64\n",
    "    ConvDown2  = keras.layers.Conv2D(filters=16,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr1)\n",
    "    Lr2        = keras.layers.LeakyReLU(alpha=0.1)(ConvDown2)\n",
    "    #32\n",
    "    ConvDown3  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr2)\n",
    "    Lr3        = keras.layers.LeakyReLU(alpha=0.1)(ConvDown3)\n",
    "    #16\n",
    "    ConvDown4  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr3)\n",
    "    Lr4        = keras.layers.LeakyReLU(alpha=0.1)(ConvDown4)\n",
    "    #8\n",
    "    ConvDown5  = keras.layers.Conv2D(filters=32,kernel_size=(2,2),strides=(2,2),padding=\"same\")(Lr4)\n",
    "    Lr5        = keras.layers.LeakyReLU(alpha=0.1)(ConvDown5)\n",
    "    #4\n",
    "\n",
    "    UpSamp1 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr5)\n",
    "    #8\n",
    "    merge1  = keras.layers.concatenate([ConvDown4,UpSamp1],axis=-1)#(UpSamp1)\n",
    "    Conv1   = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(1,1),padding=\"same\")(merge1)\n",
    "    Lr6     = keras.layers.LeakyReLU(alpha=0.1)(Conv1)\n",
    "    #8\n",
    "    UpSamp2 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr6)\n",
    "    #16\n",
    "    merge2  = keras.layers.concatenate([ConvDown3,UpSamp2],axis=-1)#(UpSamp2)\n",
    "    Conv2   = keras.layers.Conv2D(filters=32,kernel_size=(4,4),strides=(1,1),padding=\"same\")(merge2)\n",
    "    Lr7     = keras.layers.LeakyReLU(alpha=0.1)(Conv2)\n",
    "    #16\n",
    "    UpSamp3 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr7)\n",
    "\n",
    "    #32\n",
    "    Conv3   = keras.layers.Conv2D(filters=16,kernel_size=(4,4),strides=(1,1),padding=\"same\")(UpSamp3)\n",
    "    Lr8     = keras.layers.LeakyReLU(alpha=0.1)(Conv3)\n",
    "\n",
    "    UpSamp4 = keras.layers.UpSampling2D(size=(2, 2), data_format=\"channels_last\")(Lr8)\n",
    "    #64\n",
    "    Conv4   = keras.layers.Conv2D(filters=8,kernel_size=(4,4),strides=(1,1),padding=\"same\")(UpSamp4)\n",
    "    Lr9     = keras.layers.LeakyReLU(alpha=0.1)(Conv4)\n",
    "    \n",
    "    Conv5   = keras.layers.Conv2D(filters=1,kernel_size=(4,4),strides=(1,1),padding=\"same\",activation = 'tanh')(Lr9)\n",
    "    \n",
    "    return keras.models.Model(inputs=init, outputs=Conv5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ds = keras.Sequential()\n",
    "def spatial_discriminator():\n",
    "    dropout = 0.5\n",
    "    init = keras.layers.Input(shape=(64,64,1))\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(filters=4 ,kernel_size=4, strides=2, padding='same')(init)\n",
    "    relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(dropout)(relu1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(filters=8, kernel_size=4, strides=2, padding='same')(dropout1)\n",
    "    relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(dropout)(relu2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv2D(filters=16, kernel_size=4, strides=2, padding='same')(dropout2)\n",
    "    relu3 = keras.layers.LeakyReLU(alpha=0.2)(conv3)\n",
    "    dropout3 = keras.layers.Dropout(dropout)(relu3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv2D(filters=32, kernel_size=4, strides=2, padding='same')(dropout3)\n",
    "    relu4 = keras.layers.LeakyReLU(alpha=0.2)(conv4)\n",
    "    dropout4 = keras.layers.Dropout(dropout)(relu4)\n",
    "    \n",
    "    # Out: 1-dim probability\n",
    "    flatten = keras.layers.Flatten()(dropout4)\n",
    "    fcl1 = keras.layers.Dense(1)(flatten)\n",
    "    sig1 = keras.layers.Activation('sigmoid')(fcl1)\n",
    "    \n",
    "    return keras.models.Model(inputs=init, outputs=sig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def temporal_discriminator():\n",
    "    dropout = 0.5\n",
    "    init = keras.layers.Input(shape=(2,64,64,3)) # t - 1, t\n",
    "    advected = np.array(advect(init[0]),init[1])\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(filters=4 ,kernel_size=4, strides=2, padding='same')(advected)\n",
    "    relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(dropout)(relu1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(filters=8, kernel_size=4, strides=2, padding='same')(dropout1)\n",
    "    relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(dropout)(relu2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv2D(filters=16, kernel_size=4, strides=2, padding='same')(dropout2)\n",
    "    relu3 = keras.layers.LeakyReLU(alpha=0.2)(conv3)\n",
    "    dropout3 = keras.layers.Dropout(dropout)(relu3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv2D(filters=32, kernel_size=4, strides=2, padding='same')(dropout3)\n",
    "    relu4 = keras.layers.LeakyReLU(alpha=0.2)(conv4)\n",
    "    dropout4 = keras.layers.Dropout(dropout)(relu4)\n",
    "    \n",
    "    # Out: 1-dim probability\n",
    "    flatten = keras.layers.Flatten()(dropout4)\n",
    "    fcl1 = keras.layers.Dense(1)(flatten)\n",
    "    sig1 = keras.layers.Activation('sigmoid')(fcl1)\n",
    "    \n",
    "    return keras.models.Model(inputs=init, outputs=sig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advect(image): # (64,64,3)\n",
    "    padded = np.pad(image,(0,1),'edge')[:,:,:-1]\n",
    "    advected = np.empty_like(image)\n",
    "    advected[:,:,0] = image[:,:,0] + image[:,:,1]*(padded[1:,:,0] - padded[:-1,:,0])[:,:-1] + image[:,:,2]*(padded[:,1:,0] - padded[:,:-1,0])[:-1]\n",
    "    advected[:,:,1:] = image[:,:,1:]\n",
    "    return advected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#modified from source: https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.metric = [src.relative_error_tensor]\n",
    "\n",
    "        d_optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        g_optimizer = keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=d_optimizer,\n",
    "            metrics=[\"accuracy\"])\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        #self.generator.compile(loss='mse',\n",
    "        #    optimizer=g_optimizer,\n",
    "        #    metrics=self.metric)\n",
    "        \n",
    "        # The generator takes a sequence of frames as input and generates the next image in that sequence\n",
    "        input_img = keras.layers.Input(shape=self.img_shape)\n",
    "        generated = self.generator(input_img)\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.discriminator.trainable = False\n",
    "        self.combined = keras.models.Model(input_img, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=g_optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self,network=\"U-net\"):  \n",
    "        generator = keras.Sequential()\n",
    "        if network in [\"Unet\", \"U-net\", \"unet\", \"u-net\"]:\n",
    "            return unet()\n",
    "\n",
    "    def build_discriminator(self, which=\"s\"):\n",
    "        if which == \"s\":\n",
    "            return spatial_discriminator()\n",
    "        elif which == \"t\":\n",
    "            return temporal_discriminator()\n",
    "\n",
    "    def train(self, epochs, g_epochs=1, d_epochs=1, dataset=\"5min\", batch_size=128):\n",
    "\n",
    "        # Load the dataset\n",
    "        train, xval, test = src.load_datasets(dataset=dataset)\n",
    "        unet_train     = np.reshape(train[:,0,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "        unet_truth     = np.reshape(train[:,1,:,:,:],((train.shape[0],)+train.shape[2:]))\n",
    "        unet_val       = np.reshape(xval[:,0,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "        unet_val_truth = np.reshape(xval[:,1,:,:,:],((xval.shape[0],)+xval.shape[2:]))\n",
    "        print(\"Shape of training data: \",unet_train.shape,\"\\nShape of training truth: \",unet_truth.shape,\n",
    "        \"\\nShape of validation data: \",unet_val.shape,\"\\nShape of validation truth: \",unet_val_truth.shape)\n",
    "        unet_test      = np.reshape(test[:,0,:,:,:],((test.shape[0],)+test.shape[2:]))\n",
    "        unet_test_truth = np.reshape(test[:,1,:,:,:],((test.shape[0],)+test.shape[2:]))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        #store losses\n",
    "\n",
    "        log = {\"g_loss\":[],\n",
    "               \"d_loss\":[],\n",
    "               \"g_metric\":[],\n",
    "               \"d_metric\":[]}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            idx = np.random.randint(0, unet_truth.shape[0], batch_size)\n",
    "            real_imgs = unet_truth[idx]\n",
    "            training_batch = unet_train[idx]\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            generated_imgs = self.generator.predict(training_batch)\n",
    "\n",
    "            # Train the discriminator\n",
    "            for ks in range(d_epochs):\n",
    "                d_loss_real = self.discriminator.train_on_batch(real_imgs, real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(generated_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                print(f\"    {ks} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}]\")\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            idx2 = np.random.randint(0, unet_train.shape[0], batch_size)\n",
    "            training_batch = unet_train[idx2]\n",
    "            training_truth = unet_truth[idx2]\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            for kg in range(g_epochs):\n",
    "                g_loss = self.combined.train_on_batch(training_batch, real)\n",
    "\n",
    "            # Plot the progress\n",
    "            log[\"g_loss\"].append(g_loss)\n",
    "            log[\"d_loss\"].append(d_loss[0])\n",
    "            #log[\"g_metric\"].append(g_loss[1])\n",
    "            log[\"d_metric\"].append(d_loss[1])\n",
    "            print(f\"\\033[1m {epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}]\\033[0m\"+\n",
    "                  f\"\\033[1m[G loss: {g_loss}]\\033[0m\")#, rel. err.: {g_loss[1]}] \\033[0m\")\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch in [int(x) for x in np.linspace(0.1,1,10)*epochs]:\n",
    "                self.sample_images(epoch, unet_test, unet_test_truth)\n",
    "        return log\n",
    "\n",
    "    def sample_images(self, epoch, unet_test, unet_test_truth):\n",
    "        n = 5\n",
    "        test_batch = unet_test[:n]\n",
    "        test_truth = unet_test_truth[:n]\n",
    "        gen_imgs = self.generator.predict(test_batch)\n",
    "\n",
    "        fig, axs = plt.subplots(n, 3, figsize=(16, 16))\n",
    "        for i in range(n):\n",
    "                axs[i,0].imshow(test_batch[i, :,:,0])\n",
    "                axs[i,0].axis('off')\n",
    "                axs[i,0].set_title(\"Frame t\")\n",
    "                axs[i,1].imshow(test_truth[i, :,:,0])\n",
    "                axs[i,1].axis('off')\n",
    "                axs[i,1].set_title(\"Frame t+1\")\n",
    "                axs[i,2].imshow(gen_imgs[i, :,:,0])\n",
    "                axs[i,2].axis('off')\n",
    "                axs[i,2].set_title(\"Prediction t+1\")\n",
    "        fig.savefig(\"Plots/epoch %d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    log = gan.train(epochs=10, d_epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = plt.figure(figsize=(16,4))\n",
    "ax = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax.plot(log[\"g_loss\"],label=\"Generator loss\")\n",
    "ax.plot(log[\"d_loss\"],label=\"Discriminator loss\")\n",
    "ax.grid()\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(log[\"g_metric\"],label=\"Generator metric\")\n",
    "ax2.plot(log[\"d_metric\"],label=\"Discriminator metric\")\n",
    "ax2.grid()\n",
    "ax2.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
