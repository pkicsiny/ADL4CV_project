{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get rain data and visualise for overview <font color='orange'>&#9728;</font>\n",
    "- cut and split data to reasonable train, val, test sets <font color='orange'>&#9728;</font>\n",
    "- set up first generator and discriminator and overfit, play around with it, get first results (_November/December_)\n",
    "- train on small training set and improve losses, accuracy, results (_December_)\n",
    "- experiment with temporal discriminator (_December/January_)\n",
    "- experiment with wind data (_January_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data/rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization, TimeDistributed, LSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forces CPU usage\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________-\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain measurements\n",
    "Measurements are downloaded from the DWD (German weather service) open data server: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/__<br>\n",
    "I'm working with the data of August 2010 (based on [this](https://tradingeconomics.com/germany/precipitation)), so I have downloaded this: __ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/2010/RW-201008.tar__<br>\n",
    "_DWD manual:_<br>\n",
    "__ftp://ftp-cdc.dwd.de/pub/CDC/grids_germany/hourly/radolan/historical/asc/BESCHREIBUNG_gridsgermany_hourly_radolan_historical_asc_de.pdf__<br><br>\n",
    "This contains radar maps recorded in every hour. Each map has a resolution of $900\\times900$ pixels and each pixel corresponds to an $1\\,km\\times1\\,km$ area in reality. Pixel values are the precipitation height in $0.1\\,mm$.\n",
    "Below I'm importing the data of this as a series of numpy arrays and plot them to see the acual radar map. The _sys.path[0]_ is the path on my computer and it can be different for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = src.get_data(sys.path[0], which=\"5m\", total_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = src.generate_datasets(inputs, n=10, size=64, length=2, normalize=True, split=(6,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(sys.path[0]+\"/5_minute.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_res_train = images[\"low_res_train\"]\n",
    "#low_res_xval = images[\"low_res_xval\"]\n",
    "#low_res_test = images[\"low_res_test\"]\n",
    "#overfit = np.reshape(images['images'],np.shape(images['images'])+(1,))\n",
    "train = np.reshape(images[\"train\"],np.shape(images[\"train\"])+(1,))\n",
    "xval = np.reshape(images[\"xval\"],np.shape(images[\"xval\"])+(1,))\n",
    "test = np.reshape(images[\"test\"],np.shape(images[\"test\"])+(1,))\n",
    "print(f\"Training data: {train.shape}\\nValidation data: {xval.shape}\\nTest data: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train     = np.reshape(train[:,0,:,:,:],((train.shape[0],1,)+train.shape[2:]))\n",
    "lstm_truth     = np.reshape(train[:,1,:,:,:],((train.shape[0],1,)+train.shape[2:]))\n",
    "lstm_val       = np.reshape(xval[:,0,:,:,:],((xval.shape[0],1,)+xval.shape[2:]))\n",
    "lstm_val_truth = np.reshape(xval[:,1,:,:,:],((xval.shape[0],1,)+xval.shape[2:]))\n",
    "print(lstm_train.shape,\"\\n\",lstm_truth.shape,\"\\n\",lstm_val.shape,\"\\n\",lstm_val_truth.shape)\n",
    "lstm_test      = np.reshape(test[:,0,:,:,:],((test.shape[0],1,)+test.shape[2:]))\n",
    "lstm_test_truth = np.reshape(test[:,1,:,:,:],((test.shape[0],1,)+test.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_train = np.reshape(train[5,0,:,:,:],((1,1,)+train.shape[2:]))\n",
    "overfit_truth = np.reshape(train[5,1,:,:,:],((1,1,)+train.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "src.visualise_data(xval[:,:,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#batch_size=10\n",
    "lstm_model = keras.Sequential()\n",
    "# define CNN model\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=8, kernel_size=5, strides=2, padding='same'))) #8\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "lstm_model.add(keras.layers.Activation(\"relu\"))\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=16, kernel_size=5, strides=2, padding='same')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "lstm_model.add(keras.layers.Activation(\"relu\"))\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=5, strides=2, padding='same')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "lstm_model.add(keras.layers.Activation(\"relu\"))\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=5, strides=2, padding='same')))\n",
    "lstm_model.add(keras.layers.BatchNormalization())\n",
    "lstm_model.add(keras.layers.Activation(\"relu\"))\n",
    "\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(\n",
    "    filters=128, kernel_size=5, strides=4, padding='same', activation='tanh')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "\n",
    "#LSTM\n",
    "lstm_model.add(keras.layers.LSTM(units=128, return_sequences=True))\n",
    "\n",
    "#upconv\n",
    "lstm_model.add(keras.layers.Reshape((1,1,1,128)))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=64,kernel_size=5,strides=4,padding='same', activation='relu'))) #64\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=32,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=16,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=8,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "lstm_model.add(keras.layers.TimeDistributed(keras.layers.Conv2DTranspose(\n",
    "    filters=1,kernel_size=5,strides=2,padding='same', activation='relu')))\n",
    "\n",
    "lstm_model.build((None,1,)+train.shape[2:])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "            keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),loss=\"mean_squared_error\", metrics=[src.relative_error_tensor]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "lstm_model.fit(lstm_train, lstm_truth,\n",
    "               validation_data=(lstm_val, lstm_val_truth),\n",
    "               batch_size = 100,\n",
    "               epochs=150,\n",
    "               callbacks=callback,\n",
    "               shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = lstm_model.history\n",
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'],)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.savefig('training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = lstm_model.predict(lstm_test, batch_size=50)\n",
    "truth       = lstm_test_truth\n",
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = src.arg_getter(truth, predictions)\n",
    "args[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_images, error_vals, error_means = src.error_distribution(truth,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src.result_plotter([0], (lstm_test[:,0,:,:,0], truth[:,0,:,:,0], predictions[:,0,:,:,0], error_images[:,0,:,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = keras.Sequential()\n",
    "depth = 2\n",
    "dropout = 0.4\n",
    "\n",
    "conv1 = keras.layers.Conv2D(filters=depth*1 ,kernel_size=10, strides=5, input_shape=(900,900,1), padding='same')\n",
    "relu1 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout1 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv2 = keras.layers.Conv2D(filters=depth*2, kernel_size=10, strides=5, padding='same')\n",
    "relu2 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout2 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv3 = keras.layers.Conv2D(filters=depth*4, kernel_size=6, strides=2, padding='same')\n",
    "relu3 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout3 = keras.layers.Dropout(dropout)\n",
    "\n",
    "conv4 = keras.layers.Conv2D(filters=depth*8, kernel_size=2, strides=2, padding='same')\n",
    "relu4 = keras.layers.LeakyReLU(alpha=0.2)\n",
    "dropout4 = keras.layers.Dropout(dropout)\n",
    "\n",
    "# Out: 1-dim probability\n",
    "flatten = keras.layers.Flatten()\n",
    "fcl1 = keras.layers.Dense(1)\n",
    "sig1 = keras.layers.Activation('sigmoid')\n",
    "#summary()\n",
    "D.add(conv1)\n",
    "D.add(relu1)\n",
    "D.add(dropout1)\n",
    "D.add(conv2)\n",
    "D.add(relu2)\n",
    "D.add(dropout2)\n",
    "D.add(conv3)\n",
    "D.add(relu3)\n",
    "D.add(dropout3)\n",
    "D.add(conv4)\n",
    "D.add(relu4)\n",
    "D.add(dropout4)\n",
    "D.add(flatten)\n",
    "D.add(fcl1)\n",
    "D.add(sig1)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=3e-8)\n",
    "AM = keras.Sequential()\n",
    "AM.add(G)\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM.fit(train_val_inputs,\n",
    "          train_val_targets,\n",
    "          batch_size = 50,\n",
    "          epochs=50,\n",
    "          validation_split = 0.1,\n",
    "          shuffle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
