{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/trainings')\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')\n",
    "#forces CPU usage\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, gan_test, gan_test_truth, past_input):\n",
    "    n = 5\n",
    "    test_batch = gan_test[:n]\n",
    "    test_truth = gan_test_truth[:n]\n",
    "    gen_imgs = generator.predict(test_batch)\n",
    "    plot_range = past_input \n",
    "    fig, axs = plt.subplots(n, plot_range+2, figsize=(16, 16))\n",
    "    for i in range(n):\n",
    "        vmax = np.max([np.max(test_batch[i]), np.max(test_truth[i])])\n",
    "        vmin = 0\n",
    "        for j in range(plot_range):\n",
    "            im = axs[i,j].imshow(test_batch[i, :,:,j], vmax=vmax,vmin=vmin)\n",
    "            axs[i,j].axis('off')\n",
    "            src.colorbar(im)\n",
    "            axs[i,j].set_title(\"Frame t\"+str([-past_input+1+j if j < past_input-1 else \"\"][0]))\n",
    "        im2 = axs[i,-2].imshow(test_truth[i, :,:,0], vmax=vmax, vmin=vmin)\n",
    "        axs[i,-2].axis('off')\n",
    "        src.colorbar(im2)                \n",
    "        axs[i,-2].set_title(\"Frame t+1\")\n",
    "        im3 = axs[i,-1].imshow(gen_imgs[i, :,:,0], vmax=vmax, vmin=vmin)\n",
    "        axs[i,-1].axis('off')\n",
    "        src.colorbar(im3)\n",
    "        axs[i,-1].set_title(\"Prediction t+1\")\n",
    "    fig.savefig(\"Plots/epoch %d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 2\n",
    "name = f\"sgan_{past}-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, xval, test = src.load_datasets(past_frames=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train, gan_truth, gan_val, gan_val_truth, gan_test, gan_test_truth = src.split_datasets(\n",
    "            train[:2000], xval, test, past_frames=past, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make discriminator labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial ground truths\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "#Generator ground truths\n",
    "g_real = np.ones((batch_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make generator but don't compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = src.unet(gan_train.shape[1:], dropout=0, batchnorm=True, kernel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make discriminator and compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = src.spatial_discriminator(condition_shape=gan_train.shape[1:], dropout = 0.25, batchnorm=True)\n",
    "discriminator.compile(loss=keras.losses.binary_crossentropy,optimizer=keras.optimizers.SGD(),\n",
    "                      metrics=[keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and outputs of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_t = keras.layers.Input(shape=gan_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generator(frame_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = discriminator([frame_t, generated])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze discriminator weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights=[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.models.Model(inputs=[frame_t], outputs=[generated, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined.compile(loss=[src.custom_loss(loss=\"l1\"), keras.losses.binary_crossentropy], optimizer=keras.optimizers.Adam(0.0002, 0.5),\n",
    "                 loss_weights=loss_weights, metrics=[src.relative_error_tensor,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = {\"g_loss\":[],\n",
    "       \"d_loss\":[],\n",
    "       \"g_metric\":[],\n",
    "       \"d_metric\":[],\n",
    "       \"d_loss_real\":[],\n",
    "       \"d_loss_fake\":[],\n",
    "       \"d_test_real\":[],\n",
    "       \"d_test_fake\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train x epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    discriminator.trainable = True\n",
    "    idx = np.random.randint(0, gan_truth.shape[0], batch_size)\n",
    "    real_imgs = gan_truth[idx]\n",
    "    training_batch = gan_train[idx]\n",
    "        \n",
    "    generated_imgs = generator.predict(training_batch) \n",
    "    d_loss_real = discriminator.train_on_batch([training_batch, real_imgs], real)\n",
    "    d_loss_fake = discriminator.train_on_batch([training_batch, generated_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    discriminator.trainable = False \n",
    "    \n",
    "    idx = np.random.randint(0, gan_train.shape[0], batch_size)\n",
    "    training_batch = gan_train[idx]\n",
    "    training_truth = gan_truth[idx]\n",
    "    \n",
    "    g_loss = combined.train_on_batch(training_batch, [training_truth, g_real])\n",
    "    \n",
    "    if g_loss[1] < 0.11 and loss_weights[0] > 2**(-4):\n",
    "        loss_weights[0] /= 2\n",
    "        combined.compile(loss=[src.custom_loss(loss=\"l1\"), keras.losses.binary_crossentropy], optimizer=keras.optimizers.Adam(0.0002, 0.5),\n",
    "                 loss_weights=loss_weights)\n",
    "    \n",
    "    log[\"g_loss\"].append(g_loss) #sum, obj, bce\n",
    "    log[\"d_loss\"].append(d_loss) #sum\n",
    "    log[\"g_metric\"].append(g_loss[1])\n",
    "    log[\"d_metric\"].append(d_loss[1])\n",
    "    log[\"d_loss_real\"].append(d_loss_real)\n",
    "    log[\"d_loss_fake\"].append(d_loss_fake)\n",
    "    \n",
    "    print(f\"\\033[1m {epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}]\\033[0m \\n\"+\n",
    "          f\"\\033[1m {epoch} [G loss: {g_loss[0]}, G obj.: {g_loss[1]}, G bce.: {g_loss[2]}]\\033[0m \\n\"+\n",
    "          f\"\\033[1m {epoch} [real loss: {d_loss_real}, fake loss: {d_loss_fake}]\\033[0m\")\n",
    "    if epoch%100 == 0:\n",
    "        sample_images(epoch, gan_test, gan_test_truth, past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "plt.plot(np.array(log[\"g_loss\"])[:,0], alpha=0.3,c=\"b\")\n",
    "plt.plot(np.array(log[\"d_loss\"])[:,0],alpha=0.3, c=\"orange\")\n",
    "\n",
    "plt.plot(np.array(log[\"g_loss\"])[:,1], alpha=0.9,c=\"green\", label=\"L1 objective\")\n",
    "\n",
    "plt.plot(src.smooth(np.array(log[\"g_loss\"])[:,0]),c=\"b\", label=\"generator\")\n",
    "plt.plot(src.smooth(np.array(log[\"d_loss\"])[:,0]),c=\"orange\", label=\"discriminator\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"sGAN_training_curves_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_g_loss = np.array(log[\"g_loss\"])[:,0]\n",
    "total_d_loss = np.array(log[\"d_loss\"])[:,0]\n",
    "smoothed_tgl = src.smooth(np.array(log[\"g_loss\"])[:,0])\n",
    "smoothed_tdl = src.smooth(np.array(log[\"d_loss\"])[:,0])\n",
    "objective_loss = np.array(log[\"g_loss\"])[:,1]\n",
    "\n",
    "# plot 'em\n",
    "f, (a0, a1) = plt.subplots(2,1, gridspec_kw = {'height_ratios':[5, 2]})\n",
    "a0.plot(total_g_loss, alpha=0.3, c=\"b\")\n",
    "a0.plot(total_d_loss, alpha=0.3, c=\"orange\")\n",
    "a0.plot(smoothed_tgl, c=\"b\", label=\"generator\")\n",
    "a0.grid()\n",
    "a0.plot(smoothed_tdl, c=\"orange\", label=\"discriminator\")\n",
    "a0.legend()\n",
    "a1.plot(objective_loss, alpha=0.9, c=\"green\", label=\"L1 objective\")\n",
    "a1.grid()\n",
    "a1.legend()\n",
    "f.text(0.5, 0, 'Iterations', ha='center', va='center')\n",
    "f.text(0, 0.5, 'Loss', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(name+'_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_log\",log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.save_weights(name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.load_weights(\"C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/\"+name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, xval, test = src.load_datasets(past_frames=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = src.augment_data(test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predictions = {}\n",
    "past_frames = test_data[...,0:past]\n",
    "test_truth = test_data[...,past:past+1]\n",
    "for t in range(4): #predict 4 next\n",
    "    future = combined.predict(past_frames, batch_size=64)\n",
    "    predictions[f\"{t}\"] = future[0]\n",
    "    predictions[f\"{t}_labels\"] = future[1]\n",
    "    past_frames = np.concatenate((past_frames[:,:,:,1:], predictions[f\"{t}\"]), axis=-1)\n",
    "    test_truth = test_data[...,past+1+t:past+2+t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_examples(name, test, predictions_dict, past, samples=0):\n",
    "    fig, axs = plt.subplots(len(samples)*2,past+4, figsize=(32, 32))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.0)\n",
    "    for n in range(len(samples)):\n",
    "        vmax = np.max(test[n,:,:,:past])\n",
    "        vmin = 0\n",
    "        print(test.shape)\n",
    "        for i in range(past):\n",
    "            im = axs[2*n,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax,vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Past frame {i+1}\")\n",
    "            src.colorbar(im)\n",
    "        for i in range(past,past+4):\n",
    "            im = axs[2*n,i].imshow(predictions_dict[f\"{i-past}\"][samples[n], :,:,0], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n,i].axis('off')\n",
    "            axs[2*n,i].set_title(f\"Predicted frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "            im = axs[2*n+1,i].imshow(test[samples[n], :,:,i], vmax=vmax, vmin=vmin)\n",
    "            axs[2*n+1,i].axis('off')\n",
    "            axs[2*n+1,i].set_title(f\"Reference frame {i-past+1}\")\n",
    "            src.colorbar(im)\n",
    "    fig.savefig(f\"Plots/{name}_sequence_prediction.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_examples(name, test_data, predictions, past, samples=[33,46,54])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(predictions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.load(sys.path[0]+\"/5min_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *4 bc of augmentaion (it concats the frames so the 0th 1000th 2000th and 3000th are the same sample just rotated)\n",
    "test_norms = list(norms[9000:])*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize test samples\n",
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(test_data)])\n",
    "renormalized_predictions = np.transpose((np.array([[sample * np.array(test_norms)[i] for i, sample in enumerate(predictions[key])] for key in ['0', '1', '2', '3']])[:,:,:,:,0]), (1,2,3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renormalized_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds: 2, 8, 42\n",
    "thresholds = [10, 50, 100]\n",
    "scores = {}\n",
    "for t in range(renormalized_predictions.shape[-1]): # loop over the predictions (4)\n",
    "    for s in thresholds: # make a dict entry for each threshold score\n",
    "        scores[f\"pred_{t+1}_threshold_{s}\"] = src.calculate_skill_scores(renormalized_predictions[...,t:t+1],\n",
    "                                                                                     renormalized_test[...,past+t:past+1+t],\n",
    "                                                                                     x=renormalized_test[...,:past],\n",
    "                                                                                     threshold=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"pred_1_threshold_10\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_scores\",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_scores = np.load(sys.path[1]+\"/\"+name+\"/\"+name+\"_scores.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loaded_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_4_threshold_100\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_2_threshold_100\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_3_threshold_100\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((pd.Series(scores[\"pred_4_threshold_100\"][\"corr_to_truth\"]).dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"Calculates the Wasserstein loss for a sample batch.\n",
    "    The Wasserstein loss function is very simple to calculate. In a standard GAN, the discriminator\n",
    "    has a sigmoid output, representing the probability that samples are real or generated. In Wasserstein\n",
    "    GANs, however, the output is linear with no activation function! Instead of being constrained to [0, 1],\n",
    "    the discriminator wants to make the distance between its output for real and generated samples as large as possible.\n",
    "    The most natural way to achieve this is to label generated samples -1 and real samples 1, instead of the\n",
    "    0 and 1 used in normal GANs, so that multiplying the outputs by the labels will give you the loss immediately.\n",
    "    Note that the nature of this loss means that it can be (and frequently will be) less than 0.\"\"\"\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_d_labels(real, fake):\n",
    "    # idea: https://arxiv.org/pdf/1606.03498.pdf\n",
    "    batch_size = len(real)\n",
    "    five_percent = int(0.05*batch_size)\n",
    "    idx = np.random.randint(0, batch_size, five_percent)\n",
    "    d_real = np.ones_like(real)\n",
    "    d_fake = np.zeros_like(fake)\n",
    "    d_real[idx] = 0\n",
    "    d_fake[idx] = 1\n",
    "    return d_real, d_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
