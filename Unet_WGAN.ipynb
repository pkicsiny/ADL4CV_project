{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/trainings')\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')\n",
    "import src\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import math\n",
    "import io\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from  matplotlib.animation import FuncAnimation\n",
    "from matplotlib import colors\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import clear_output\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = src.load_datasets(past_frames=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, T_train, X_val, T_val, X_test, T_test = src.split_datasets(\n",
    "            train[:2000], val, test, past_frames=2, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP\n",
    "    reference sources:\n",
    "* https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "* https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html\n",
    "* https://github.com/bobchennan/Wasserstein-GAN-Keras/blob/master/mnist_wacgan.py\n",
    "* https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Loss functions for WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "    \"\"\"Calculates the gradient penalty loss for a batch of \"averaged\" samples.\n",
    "    In Improved WGANs, the 1-Lipschitz constraint is enforced by adding a term to the loss function\n",
    "    that penalizes the network if the gradient norm moves away from 1. However, it is impossible to evaluate\n",
    "    this function at all points in the input space. The compromise used in the paper is to choose random points\n",
    "    on the lines between real and generated samples, and check the gradients at these points. Note that it is the\n",
    "    gradient w.r.t. the input averaged samples, not the weights of the discriminator, that we're penalizing!\n",
    "    In order to evaluate the gradients, we must first run samples through the generator and evaluate the loss.\n",
    "    Then we get the gradients of the discriminator w.r.t. the input averaged samples.\n",
    "    The l2 norm and penalty can then be calculated for this gradient.\n",
    "    Note that this loss function requires the original averaged samples as input, but Keras only supports passing\n",
    "    y_true and y_pred to loss functions. To get around this, we make a partial() of the function with the\n",
    "    averaged_samples argument, and use that for model training.\"\"\"\n",
    "    # first get the gradients:\n",
    "    #   assuming: - that y_pred has dimensions (batch_size, 1)\n",
    "    #             - averaged_samples has dimensions (batch_size, nbr_features)\n",
    "    # gradients afterwards has dimension (batch_size, nbr_features), basically\n",
    "    # a list of nbr_features-dimensional gradient vectors\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Discriminator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Spatial_D(input_shape=(64, 64, 1), condition_shape=(64, 64, 1), dropout=0.3, batchnorm=False):\n",
    "    # condition is the frame t (the original frame) or the sequence of past frames\n",
    "    condition = Input(shape=condition_shape)\n",
    "    # other is the generated prediction of frame t+1 or the ground truth frame t+1\n",
    "    other = Input(shape=input_shape)\n",
    "    # Concatenate image and conditioning image by channels to produce input\n",
    "    combined_imgs = Concatenate(axis=-1)([condition, other])\n",
    "\n",
    "    conv1 = Conv2D(filters=16, kernel_size=4, strides=2, padding='same')(combined_imgs)\n",
    "    if batchnorm:\n",
    "        conv1   = BatchNormalization()(conv1)\n",
    "    relu1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        relu1 = Dropout(dropout)(relu1)\n",
    "\n",
    "    conv2 = Conv2D(filters=32, kernel_size=4, strides=2, padding='same')(relu1)\n",
    "    if batchnorm:\n",
    "        conv2   = BatchNormalization()(conv2)\n",
    "    relu2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        relu2 = Dropout(dropout)(relu2)\n",
    "\n",
    "    conv3 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(relu2)\n",
    "    if batchnorm:\n",
    "        conv3   = BatchNormalization()(conv3)\n",
    "    relu3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        relu3 = Dropout(dropout)(relu3)\n",
    "\n",
    "    conv4 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(relu3)\n",
    "    if batchnorm:\n",
    "        conv4   = BatchNormalization()(conv4)\n",
    "    relu4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "    if (dropout > 0) and (dropout <= 1):\n",
    "        relu4 = Dropout(dropout)(relu4)\n",
    "\n",
    "    # Out: 1-dim probability\n",
    "    flatten = Flatten()(relu4)\n",
    "    fcl1 = Dense(1)(flatten)\n",
    "    # sig1 = Activation('sigmoid', name=\"s_disc_output\")(fcl1)\n",
    "    # For WGAN, there is no activation function\n",
    "    lin1 = Activation('linear', name=\"s_disc_output\")(fcl1)\n",
    "\n",
    "    return Model(inputs=[condition, other], outputs=lin1, name='SD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT_PENALTY_WEIGHT = 10  # As per the paper\n",
    "\n",
    "# random seed\n",
    "RND = 777\n",
    "np.random.seed(RND)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "ITERATIONS = 20000\n",
    "D_ITERS = 10\n",
    "\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = build_Spatial_D(batchnorm=True)\n",
    "\n",
    "D.compile(\n",
    "    optimizer = RMSprop(lr=0.00005),\n",
    "    loss = wasserstein_loss)\n",
    "\n",
    "condition = Input(shape=(64, 64, 1), name='input_condition_')\n",
    "other = Input(shape=(64, 64, 1),name='input_other_')\n",
    "\n",
    "G = build_G_Unet(batchnorm=True)\n",
    "\n",
    "# create combined D(G) model\n",
    "output_is_fake = D(inputs = [condition, G(condition)])\n",
    "DG = Model(condition, outputs=output_is_fake)\n",
    "\n",
    "DG.summary()\n",
    "\n",
    "DG.compile(\n",
    "    optimizer = RMSprop(lr=0.00005),\n",
    "    loss = wasserstein_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "progress_bar = Progbar(target=ITERATIONS)\n",
    "\n",
    "DG_losses = []\n",
    "D_true_losses = []\n",
    "D_fake_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(ITERATIONS):\n",
    "\n",
    "    if len(D_true_losses) > 0:\n",
    "        progress_bar.update(\n",
    "            it,\n",
    "            values=[ # avg of 5 most recent\n",
    "                    ('D_real_is_fake', np.mean(D_true_losses[-5:], axis=0)),\n",
    "                    ('D_fake_is_fake', np.mean(D_fake_losses[-5:], axis=0)),\n",
    "                    ('D(G)_is_fake', np.mean(DG_losses[-5:],axis=0))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        progress_bar.update(it)\n",
    "        \n",
    "    # 1: train D on real+generated images\n",
    "\n",
    "    if (it % 1000) < 25 or it % 500 == 0: # 25 times in 1000, every 500th\n",
    "        d_iters = 100\n",
    "    else:\n",
    "        d_iters = D_ITERS\n",
    "\n",
    "    for d_it in range(d_iters):\n",
    "\n",
    "        # unfreeze D\n",
    "        D.trainable = True\n",
    "        for l in D.layers: l.trainable = True\n",
    "\n",
    "        # clip D weights\n",
    "\n",
    "        for l in D.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -0.01, 0.01) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "        # 1.1: maximize D output on reals === minimize -1*(D(real))\n",
    "\n",
    "        # draw random samples from real images\n",
    "        index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "        base_images = X_train[index]\n",
    "        real_images = T_train[index]\n",
    "\n",
    "        D_loss = D.train_on_batch([base_images, real_images], -np.ones(BATCH_SIZE))\n",
    "        #print(D_loss)\n",
    "        D_true_losses.append(D_loss)\n",
    "\n",
    "        # 1.2: minimize D output on fakes \n",
    "\n",
    "        generated_images = G.predict(base_images)\n",
    "\n",
    "        D_loss = D.train_on_batch([base_images, generated_images], np.ones(BATCH_SIZE))\n",
    "        #print(D_loss)\n",
    "        D_fake_losses.append(D_loss)\n",
    "\n",
    "    # 2: train D(G) (D is frozen)\n",
    "    # minimize D output while supplying it with fakes, \n",
    "    # telling it that they are reals (-1)\n",
    "\n",
    "    # freeze D\n",
    "    D.trainable = False\n",
    "    for l in D.layers: l.trainable = False\n",
    "        \n",
    "    index = np.random.choice(len(X_train), BATCH_SIZE, replace=False)\n",
    "    base_images = X_train[index]\n",
    "\n",
    "    DG_loss = DG.train_on_batch(base_images,-np.ones(BATCH_SIZE))\n",
    "    DG_losses.append(DG_loss)\n",
    "\n",
    "#    if it % 10 == 0:\n",
    "#        update_tb_summary(it, write_sample_images=(it % 250 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.save_weights('/Users/jlee/Desktop/JONG/TUM/18W/Advanced_Deep_Learning_for_Computer_Vision/project/results/wgan_results/iter_20000_RMSprop_0.00005/G_model.h5')\n",
    "D.save_weights('/Users/jlee/Desktop/JONG/TUM/18W/Advanced_Deep_Learning_for_Computer_Vision/project/results/wgan_results/iter_20000_RMSprop_0.00005/D_model.h5')\n",
    "DG.save_weights('/Users/jlee/Desktop/JONG/TUM/18W/Advanced_Deep_Learning_for_Computer_Vision/project/results/wgan_results/iter_20000_RMSprop_0.00005/DG_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_imgs = G.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(T_test[1,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G_imgs[1,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,4))\n",
    "ax = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "g_labels = [\"Generator loss\"]\n",
    "#loop over gen loss components\n",
    "#for curve in range(np.shape(gan.log[\"g_loss\"])[-1]):\n",
    "ax.plot(DG_losses, c=\"g\", label=\"G_loss\")\n",
    "\n",
    "ax.plot(D_true_losses, alpha=0.3, c=\"b\")\n",
    "ax.plot(src.smooth(D_true_losses) ,label=\"D_true_loss\", c=\"b\")\n",
    "ax.plot(D_fake_losses, alpha=0.6, c=\"orange\")\n",
    "ax.plot(src.smooth(D_fake_losses),label=\"D_fake_loss\", c=\"orange\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "#plt.savefig(f\"best_GAN_training_1_{gan.g_dropout}_{gan.d_dropout}.png\") #g dropout, d dropout, g batchnorm, noisy labels, loss weights, augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"src.py\", \"/Users/jlee/Desktop/JONG/TUM/18W/Advanced_Deep_Learning_for_Computer_Vision/project/ADL4CV_project/src.py\")\n",
    "src = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(src)\n",
    "#import importlib as imp\n",
    "#imp.reload(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images, error_vals, error_means = src.error_distribution(T_test, G_imgs, metric=\"difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.result_plotter(range(5), (X_test[:,:,:,0], T_test[:,:,:,0], G_imgs[:,:,:,0], error_images[:,:,:,0]), save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
