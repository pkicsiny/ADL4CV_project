{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the datasets from here: https://drive.google.com/open?id=1mNEblJS0622w5-2mB6yCyMbu7RbcfENL<br>\n",
    "This should contain the following:\n",
    "- __5_min_train__: training data as np array. Shape: (7500, 64, 64, 8)\n",
    "- __5_min_xval__: validation data (currently unused) as np array. Shape: (1500, 64, 64, 8)\n",
    "- __5_min_test__: test data (used as visual validation during training) as np array. Shape: (1000, 64, 64, 8)\n",
    "- __5_min_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence. Shape: (10000,)\n",
    "- __5_min_long_pred__: test data for sequence prediction as np array. We used it for testing after training. Shape: (1000, 64, 64, 20)\n",
    "- __5_min_long_pred_norms__: list of floats containing the maximum pixel intensity value prior to normalization for each sequence for the 5_min_long_pred dataset. Shape: (1000,)\n",
    "- __tgan_1/2/4-1_vx/vy_2000__: optical flow images between the last and second last frames of the input for the first 2000 sequences of the training dataset (__5_min_train__) as np array. The 1/2/4 means the length of the input sequence. We mostly used 2. Shape: (2000, 64, 64, 1)\n",
    "- __germany__: Not needed. (GPS coordinates of Germany. Used for experimenting before.)\n",
    "\n",
    "In the datasets the first axis is stands for the sample, the next two for the frame height and width and the last for the channels which is the time axis here.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#source code folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project')\n",
    "#model folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/ADL4CV_project/models/')\n",
    "#data folder\n",
    "sys.path.insert(0, 'C:/Users/pkicsiny/Desktop/TUM/3/ADL4CV/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #\"\" or \"-1\" for CPU, \"0\" for GPU\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = 2\n",
    "name = f\"tgan_{past}-1\"\n",
    "batch_size=64\n",
    "iterations = 5000\n",
    "#generator trainings in 1 iter\n",
    "g = 2\n",
    "#spatial disc trainings in 1 iter\n",
    "s = 1\n",
    "#temporal disc trainings in 1 iter\n",
    "t = 1\n",
    "#initialize random seed\n",
    "RND = 777\n",
    "np.random.seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to true if you want to use a pretrained model and load its weights from file\n",
    "use_loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make generator but don't compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = src.unet((64, 64, past), dropout=0, batchnorm=True, kernel_size=4, feature_mult=1) #ks 5 for load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make discriminators and don't compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_discriminator = src.spatial_discriminator(condition_shape=(64, 64, past), dropout = 0.5, batchnorm=True)\n",
    "t_discriminator = src.temporal_discriminator(dropout = 0.5, batchnorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and outputs of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_t = keras.layers.Input(shape=(64, 64, past))\n",
    "adv = keras.layers.Input(shape=(64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generator(frame_t)\n",
    "s_score = s_discriminator([frame_t, generated])\n",
    "t_score = t_discriminator([adv, generated])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights=[0.3, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.models.Model(inputs=[frame_t, adv], outputs=[generated, s_score, t_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile discriminators with trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_discriminator.compile(loss=keras.losses.binary_crossentropy,optimizer=keras.optimizers.SGD(),\n",
    "                      metrics=[keras.metrics.binary_accuracy])\n",
    "t_discriminator.compile(loss=keras.losses.binary_crossentropy,optimizer=keras.optimizers.SGD(),\n",
    "                      metrics=[keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze discriminator weights and compile combined GAN with only the generator trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_discriminator.trainable = False\n",
    "t_discriminator.trainable = False\n",
    "combined.compile(loss=[src.custom_loss(loss=\"l1\"),\n",
    "                       keras.losses.binary_crossentropy,\n",
    "                       keras.losses.binary_crossentropy],\n",
    "                 optimizer=keras.optimizers.Adam(0.0002, 0.5),\n",
    "                 loss_weights=loss_weights,\n",
    "                 metrics=[src.relative_error_tensor, \"accuracy\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a pretrained model load and use its weights. Also set discriminator weights to trainable then recompile model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_loaded:\n",
    "    combined.load_weights(sys.path[1]+name+\"/\"+name+\"_model.h5\")\n",
    "    generator = combined.layers[1]\n",
    "    s_discriminator = combined.layers[3]\n",
    "    s_discriminator.trainable = True\n",
    "    for l in s_discriminator.layers: l.trainable = True\n",
    "    t_discriminator = combined.layers[4]\n",
    "    t_discriminator.trainable = True\n",
    "    for l in t_discriminator.layers: l.trainable = True\n",
    "    s_discriminator.compile(loss=keras.losses.binary_crossentropy,optimizer=keras.optimizers.SGD(),\n",
    "                      metrics=[keras.metrics.binary_accuracy])\n",
    "    t_discriminator.compile(loss=keras.losses.binary_crossentropy,optimizer=keras.optimizers.SGD(),\n",
    "                      metrics=[keras.metrics.binary_accuracy])\n",
    "    s_discriminator.trainable = False\n",
    "    t_discriminator.trainable = False\n",
    "    combined.compile(loss=[src.custom_loss(loss=\"l1\"),\n",
    "                       keras.losses.binary_crossentropy,\n",
    "                       keras.losses.binary_crossentropy],\n",
    "                     optimizer=keras.optimizers.Adam(0.0002, 0.5),\n",
    "                     loss_weights=loss_weights,\n",
    "                     metrics=[src.relative_error_tensor, \"accuracy\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using loaded weights for prediction only then please skip to the __prediction__ section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past+1 is bc of temporal raining. See second presentation.\n",
    "train, xval, test = src.load_datasets(past_frames=past+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to inputs and ground truth images. Does data augmentation by rotations. Uses the first 2000 samples (bc. of memory issue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train, gan_truth, gan_val, gan_val_truth, gan_test, gan_test_truth = src.split_datasets(\n",
    "            train[:2000], xval, test, past_frames=past+1, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate optical flows between frame t-1 and t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optical flow of the augmented data of the first 2000 training images (8000 images)\n",
    "vx, vy = src.optical_flow(gan_train[:,:,:,-2:-1], gan_train[:,:,:,-1:], window_size=4, tau=1e-2, init=1) # (n,:,:,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save optical flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed(f\"{name}_vx_2000\",vx) #2000 denotes that they re the flo2 of the first 2000 samples from the training dataset\n",
    "#np.savez_compressed(f\"{name}_vy_2000\",vy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If optical flows are saved, load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx = np.load(sys.path[0]+f\"/{name}_vx_2000.npz\")[\"arr_0\"]\n",
    "vy = np.load(sys.path[0]+f\"/{name}_vy_2000.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess optical flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_optical_flow = src.normalize_flows(vx, vy)\n",
    "flows = np.transpose([[ndimage.median_filter(image[..., ch], 4) for ch in range(2)] for image in normalized_optical_flow], (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make discriminator labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial ground truths\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch 5% of labels and smooth from 1 to 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real, fake = src.noisy_d_labels(real, fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log dict. Either append to existing log or start with an empty one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_batches = int(gan_train.shape[0] / batch_size)\n",
    "\n",
    "if use_loaded:\n",
    "    log = np.load(sys.path[1]+name+\"/\"+name+\"_log.npy\").item()\n",
    "else:\n",
    "    log = {\"g_loss\":[],\n",
    "           \"d_loss\":[],\n",
    "           \"ds_loss\":[],\n",
    "           \"dt_loss\":[],\n",
    "           \"g_metric\":[],\n",
    "           \"d_metric\":[],\n",
    "           \"ds_metric\":[],\n",
    "           \"dt_metric\":[],\n",
    "           \"d_loss_real\":[],\n",
    "           \"d_loss_fake\":[],\n",
    "           \"ds_loss_real\":[],\n",
    "           \"ds_loss_fake\":[],\n",
    "           \"dt_loss_real\":[],\n",
    "           \"dt_loss_fake\":[],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for it in range(iterations):\n",
    "#batch\n",
    "#idx = range(it%nb_batches * batch_size,(it%nb_batches + 1) * batch_size)\n",
    "#DS create training batch\n",
    "    s_discriminator.trainable = True\n",
    "    for tds in range(s):\n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        #predict next frame  \n",
    "        generated_imgs = generator.predict(training_batch) \n",
    "        #train discriminator\n",
    "        ds_loss_real = s_discriminator.train_on_batch([training_batch, real_imgs], real)\n",
    "        ds_loss_fake = s_discriminator.train_on_batch([training_batch, generated_imgs], fake)\n",
    "        #add losses\n",
    "        ds_loss = 0.5 * np.add(ds_loss_real, ds_loss_fake)\n",
    "    s_discriminator.trainable = False\n",
    "\n",
    "#DT create training batch\n",
    "    t_discriminator.trainable = True\n",
    "    for tdt in range(t):\n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        \n",
    "        #predict frame t from frame t-1 and advect\n",
    "        aux_batch = gan_train[idx,:,:,:-1]\n",
    "        \n",
    "        advected_aux_gen = generator.predict(aux_batch)\n",
    "        advected_aux_truth = training_batch[:,:,:,-1:]\n",
    "        for i in range(10):\n",
    "            advected_aux_gen = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected_aux_gen, -flows[idx]), axis=-1)])\n",
    "            advected_aux_truth = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected_aux_truth, -flows[idx]), axis=-1)])\n",
    "     \n",
    "        #predict next frame\n",
    "        generated_imgs = generator.predict(training_batch) \n",
    "        #train discriminator\n",
    "        dt_loss_real = t_discriminator.train_on_batch([advected_aux_truth, real_imgs], real)\n",
    "        dt_loss_fake = t_discriminator.train_on_batch([advected_aux_gen, generated_imgs], fake)\n",
    "        #add losses\n",
    "        dt_loss = 0.5 * np.add(dt_loss_real, dt_loss_fake)\n",
    "    t_discriminator.trainable = False\n",
    "    \n",
    "#Total discriminator loss\n",
    "    d_loss = 0.5 * np.add(ds_loss, dt_loss)\n",
    "    d_loss_real = 0.5 * np.add(ds_loss_real, dt_loss_real)\n",
    "    d_loss_fake = 0.5 * np.add(ds_loss_fake, dt_loss_fake)\n",
    "        \n",
    "\n",
    "#Generator\n",
    "    for tg in range(g):\n",
    "        idx = np.random.choice(gan_train.shape[0], batch_size, replace=False)\n",
    "        real_imgs = gan_truth[idx]\n",
    "        training_batch = gan_train[idx,:,:,1:]\n",
    "        \n",
    "        aux_batch = gan_train[idx,:,:,:-1]\n",
    "        advected = generator.predict(aux_batch)\n",
    "        for i in range(10):\n",
    "            advected = np.array([src.advect(sample, order=2) for sample in np.concatenate((advected, -flows[idx]), axis=-1)])\n",
    "    \n",
    "        g_loss = combined.train_on_batch([training_batch, advected], [real_imgs, real, real])\n",
    "    \n",
    "    log[\"g_loss\"].append(g_loss)\n",
    "    log[\"d_loss\"].append(d_loss)\n",
    "    log[\"ds_loss\"].append(ds_loss[0])  \n",
    "    log[\"dt_loss\"].append(dt_loss[0])\n",
    "    log[\"g_metric\"].append(g_loss[1])\n",
    "    log[\"d_metric\"].append(d_loss[1])\n",
    "    log[\"ds_metric\"].append(ds_loss[1])\n",
    "    log[\"dt_metric\"].append(dt_loss[1])\n",
    "    log[\"d_loss_real\"].append(d_loss_real)\n",
    "    log[\"d_loss_fake\"].append(d_loss_fake)\n",
    "    log[\"ds_loss_real\"].append(ds_loss_real)\n",
    "    log[\"ds_loss_fake\"].append(ds_loss_fake)\n",
    "    log[\"dt_loss_real\"].append(dt_loss_real)\n",
    "    log[\"dt_loss_fake\"].append(dt_loss_fake)\n",
    "    \n",
    "    \n",
    "    print(f\"\\033[1m {it} [Ds loss: {ds_loss[0]}, acc.: {100*ds_loss[1]}]\\033[0m \\n\"+\n",
    "          f\"\\033[1m {it} [Dt loss: {dt_loss[0]}, acc.: {100*dt_loss[1]}]\\033[0m \\n\"+\n",
    "          f\"\\033[1m {it} [G loss: {g_loss[0]}, G obj.: {g_loss[1]}, Gs bce.: {g_loss[2]}, Gt bce.: {g_loss[3]}]\\033[0m\")\n",
    "    print(f\"S: real loss: {ds_loss_real}, fake loss: {ds_loss_fake}\")\n",
    "    print(f\"T: real loss: {dt_loss_real}, fake loss: {dt_loss_fake}\")\n",
    "    \n",
    "    if it in [499, 999]:  \n",
    "        loss_weights[0] -= 0.1\n",
    "        print(f\"L1 loss decreased. New weight: {loss_weights[0]}\")\n",
    "        combined.compile(loss=[src.custom_loss(loss=\"l1\"),\n",
    "                       keras.losses.binary_crossentropy,\n",
    "                       keras.losses.binary_crossentropy],\n",
    "                 optimizer=keras.optimizers.Adam(0.0002, 0.5),\n",
    "                 loss_weights=loss_weights,\n",
    "                 metrics=[src.relative_error_tensor, \"accuracy\", \"accuracy\"])\n",
    "\n",
    "src.sample_images(iterations, gan_test[...,1:], gan_test_truth, past, generator)\n",
    "src.plot_temporal_training_curves(log, iterations, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_log\",log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.save_weights(name+\"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads a 20 long sequence with 1000 sequence samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_test = src.load_datasets(prediction=True)\n",
    "sequence_test = src.augment_data(sequence_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = combined.layers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively predict next 5 frames (25 mins into the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_next = 5\n",
    "predictions = {}\n",
    "past_frames = sequence_test[...,0:past]\n",
    "test_truth = sequence_test[...,past:past+1]\n",
    "for t in range(n_next):\n",
    "    src.update_output(t)\n",
    "    future = generator.predict(past_frames, batch_size=64)\n",
    "    predictions[f\"{t}\"] = future\n",
    "    past_frames = np.concatenate((past_frames[:,:,:,1:], predictions[f\"{t}\"]), axis=-1)\n",
    "    test_truth = sequence_test[...,past+1+t:past+2+t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44, 110 in whole data w.o. augmenting, 33, 67, 57 in augmented of the first 100\n",
    "src.sequence_prediction_plot(name, sequence_test, predictions, past, samples=[33,67,57])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renormalize intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norms = np.load(sys.path[0]+\"/5min_long_pred_norms_compressed.npz\")[\"arr_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renormalize test samples\n",
    "renormalized_test = np.array([sample * np.array(test_norms)[i] for i, sample in enumerate(sequence_test)])\n",
    "renormalized_predictions = np.transpose((np.array([[sample * np.array(test_norms)[i] for i, sample in enumerate(predictions[key])] for key in list(map(str,np.arange(0,n_next)))])[:,:,:,:,0]), (1,2,3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renormalized_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate pixel intensities back to dBZ and from there to mm/h. <br>\n",
    "Sources: <br>\n",
    "- https://www.dwd.de/DE/leistungen/radolan/radolan_info/radolan_radvor_op_komposit_format_pdf.pdf?__blob=publicationFile&v=11 (page 10)\n",
    "- <https://web.archive.org/web/20160113151652/http://www.desktopdoppler.com/help/nws-nexrad.htm#rainfall%20rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dBZ\n",
    "dBZ_t = renormalized_test*0.5 - 32.5\n",
    "dBZ_p = renormalized_predictions*0.5 - 32.5\n",
    "#mm/h\n",
    "I_t = (0.005*10**(0.1*dBZ_t))**(0.625)\n",
    "I_p = (0.005*10**(0.1*dBZ_p))**(0.625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_scores = src.get_scores(renormalized_predictions, renormalized_test, n_next, past, thresholds_as_list=[18])\n",
    "scores = src.get_scores(I_p, I_t, n_next, past, thresholds_as_list=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(I_t[100,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(renormalized_test[100,...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(name+\"_scores\",scores)\n",
    "np.save(name+\"_intensity_scores\",intensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores[\"pred_1\"][\"corr_to_truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(intensity_scores[\"pred_1\"][\"corr_to_truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
